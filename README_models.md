# Model Training and Generating Responses

We use `jiant v2.1.1` to train our models. To learn more about `jiant`, please refer to the [documentation](https://github.com/nyu-mll/jiant/tree/v2.1.1).

## Models
We train **18 different models** in total (provided by Huggingface): `albert-xxlarge-v2`, `roberta-large`, `roberta-base`, `bert-large-uncased`, `bert-base-uncased`, `xlm-roberta-large`, and 12 `miniBERTas` models ([Warstadt et al., 2020](https://arxiv.org/pdf/2010.05358.pdf); [Zhang et al., 2020](https://arxiv.org/abs/2011.04946)) listed [here](https://huggingface.co/nyu-mll).



