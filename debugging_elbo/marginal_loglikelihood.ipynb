{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy\n",
    "import torch\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "import pyro\n",
    "import pyro.infer\n",
    "import pyro.infer.mcmc\n",
    "import pyro.distributions as dist\n",
    "import torch.distributions.constraints as constraints\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.log_normal import LogNormal\n",
    "\n",
    "from multi_virt_v2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/phumon/Documents/Research/nlu-test-sets/params_mvirt_sync/lr-0.0001-steps-$5500-alpha-lognormal-identity-dim*_theta-normal-identity_nosubsample_1.00_0.40_particles8/params.p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir='/Users/phumon/Documents/Research/nlu-test-sets/params_mvirt_sync'\n",
    "file_name=f'lr-0.0001-steps-$5500-alpha-lognormal-identity-dim*_theta-normal-identity_nosubsample_1.00_0.40_particles8/params.p'\n",
    "exp_dir=os.path.join(base_dir, file_name)\n",
    "\n",
    "\n",
    "print(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(exp_dir, dimension):\n",
    "    posterior_stats = {}\n",
    "    \n",
    "    pyro.clear_param_store()\n",
    "    pyro.get_param_store().load(exp_dir)\n",
    "    with torch.no_grad():\n",
    "        pyro_param_dict = dict(pyro.get_param_store().named_parameters())\n",
    "    \n",
    "    for k, v in pyro_param_dict.items():\n",
    "        if k == \"g mu\":\n",
    "            n_items = v.size(0)\n",
    "        elif k == \"t mu\":\n",
    "            n_models = v.size(0)\n",
    "        posterior_stats[k] = v.mean().item()\n",
    "    \n",
    "    betas = Normal(posterior_stats['b mu']*torch.ones(n_items, dimension), torch.exp(torch.tensor(posterior_stats['b logstd'])))\n",
    "    log_gamma = Normal(posterior_stats['g mu']*torch.ones(n_items), torch.exp(torch.tensor(posterior_stats['g logstd'])))\n",
    "    alphas = LogNormal(posterior_stats['a mu'] * torch.ones(n_items, dimension), torch.exp(torch.tensor(posterior_stats['a logstd'])))\n",
    "    thetas = Normal(posterior_stats['t mu'] * torch.ones(n_models, dimension), torch.exp(torch.tensor(posterior_stats['t logstd'])))\n",
    "    probs = {\n",
    "                \"beta\": betas,\n",
    "                \"log_gamma\": log_gamma,\n",
    "                \"alpha\": alphas,\n",
    "                \"theta\": thetas\n",
    "    }\n",
    "    \n",
    "    return posterior_stats, probs, n_items, n_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(item_param_std, alpha_std, dimension, n_items, n_models):\n",
    "    # Generate params\n",
    "    betas = Normal(torch.zeros(n_items, dimension), torch.tensor(item_param_std))\n",
    "    log_gamma = Normal(torch.zeros(n_items), torch.tensor(item_param_std))\n",
    "\n",
    "    alphas = LogNormal(0 * torch.ones(n_items, dimension), torch.tensor(alpha_std))\n",
    "\n",
    "    # Generate thetas\n",
    "    thetas = Normal(0 * torch.ones(n_models, dimension), torch.tensor(item_param_std))\n",
    "    probs = {\n",
    "                \"beta\": betas,\n",
    "                \"log_gamma\": log_gamma,\n",
    "                \"alpha\": alphas,\n",
    "                \"theta\": thetas\n",
    "    }\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_marginal_loglik(prior_probs, posterior_probs, dimension, n_particles=10):\n",
    "    log_weights = []\n",
    "    for i in range(n_particles):\n",
    "        #log p(x) = logsumexp(log p(x|z_k) + log p(z_k) - log q(z_k), dim=DIM) - log(K)\n",
    "        log_weight = 0\n",
    "        prior_values = {}\n",
    "        for k, dist_param in prior_probs.items():\n",
    "            prior_tmp = prior_probs[k].sample()\n",
    "            prior_values[k] = prior_tmp\n",
    "            posterior_tmp = posterior_probs[k].sample()\n",
    "            log_weight += (prior_probs[k].log_prob(prior_tmp) - posterior_probs[k].log_prob(posterior_tmp)).sum()\n",
    "        \n",
    "        gamma = sigmoid(prior_values['log_gamma'])\n",
    "        alphas = prior_values['alpha']\n",
    "        thetas = prior_values['theta']\n",
    "        betas = prior_values['beta']\n",
    "        if dimension > 1:\n",
    "            prob = gamma[None, :] + (1.0 - gamma[None, :]) * sigmoid(torch.sum(alphas[None, :, :] * (thetas[:, None] - betas[None, :]).squeeze(), dim=-1))\n",
    "        else:\n",
    "            betas=betas.squeeze()\n",
    "            gamma=gamma.squeeze()\n",
    "            alphas=alphas.squeeze()\n",
    "            thetas=thetas.squeeze()\n",
    "            prob = gamma[None, :] + (1.0 - gamma[None, :]) * sigmoid(alphas[None, :] * (thetas[:, None] - betas[None, :]))\n",
    "        lik_dist = torch.distributions.bernoulli.Bernoulli(prob)\n",
    "        lik_values = lik_dist.sample()\n",
    "        \n",
    "        \n",
    "        log_weight += lik_dist.log_prob(lik_values).sum()\n",
    "        log_weights.append(log_weight.item())\n",
    "    print(log_weights)\n",
    "    marginal = torch.logsumexp(torch.tensor(log_weights), 0) - torch.log(torch.tensor(n_particles))\n",
    "    return marginal\n",
    "    #return lik, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-15991.0166015625, -16066.56640625, -16634.5546875, -15519.5908203125, -18762.345703125, -15306.13671875, -18005.1328125, -19186.91796875, -16879.60546875, -16800.58984375]\n",
      "[-17975.08203125, -16520.216796875, -16124.12890625, -18429.7265625, -18251.59765625, -17730.220703125, -17190.837890625, -17095.607421875, -16362.701171875, -16486.322265625]\n",
      "[-15421.1728515625, -14920.5224609375, -17155.546875, -16431.419921875, -16884.353515625, -17090.876953125, -13768.7275390625, -16899.806640625, -11728.8037109375, -14935.193359375]\n",
      "[-16810.115234375, -18892.05859375, -16427.779296875, -15869.2353515625, -16938.8984375, -17039.359375, -14880.25, -15419.775390625, -15154.0205078125, -16090.96875]\n",
      "[-16550.1328125, -18577.87109375, -16685.1328125, -15252.1494140625, -15334.419921875, -15568.2001953125, -16942.66015625, -17039.7265625, -18487.55078125, -16938.38671875]\n",
      "[-15709.982421875, -15500.9638671875, -16291.2080078125, -17359.583984375, -16128.974609375, -15467.611328125, -14958.109375, -16897.84375, -16825.62109375, -16442.642578125]\n",
      "[-15163.0107421875, -16691.92578125, -16307.03515625, -18904.326171875, -14023.18359375, -15810.5078125, -13381.822265625, -15836.984375, -14674.396484375, -14356.32421875]\n",
      "[-18519.4609375, -18354.0625, -18059.484375, -19165.650390625, -17734.59375, -18119.0859375, -17739.806640625, -18428.78125, -17867.30859375, -17334.908203125]\n"
     ]
    }
   ],
   "source": [
    "marginal_logprob_list=[]\n",
    "correct_list=[]\n",
    "dim_list=[]\n",
    "item_param_std=1; alpha_std=0.4\n",
    "\n",
    "for filepath in glob.iglob(exp_dir):\n",
    "    dimension = int(filepath.split('/')[-2].split('_')[0].rsplit('-',1)[1][3:])\n",
    "    dim_list.append(dimension)\n",
    "    \n",
    "    posterior_stats, posterior_probs, n_items, n_models = get_posterior(filepath, dimension)\n",
    "    prior_probs = get_prior(item_param_std, alpha_std, dimension, n_items, n_models)\n",
    "    marginal = get_marginal_loglik(prior_probs, posterior_probs, dimension)\n",
    "    \n",
    "    marginal_logprob_list.append(marginal.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, -17337.2109375),\n",
       " (2, -16126.431640625),\n",
       " (3, -15254.4521484375),\n",
       " (4, -15308.439453125),\n",
       " (5, -14882.552734375),\n",
       " (6, -14960.412109375),\n",
       " (7, -13384.125),\n",
       " (8, -11731.1064453125)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_data = list(zip(dim_list, marginal_logprob_list))\n",
    "plot_data.sort(key=lambda x: x[0])\n",
    "dim, marginals = list(zip(*plot_data))\n",
    "\n",
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, -17337.2109375),\n",
       " (2, -16126.431640625),\n",
       " (3, -15254.4521484375),\n",
       " (4, -15308.439453125),\n",
       " (5, -14882.552734375),\n",
       " (6, -14960.412109375),\n",
       " (7, -13384.125),\n",
       " (8, -11731.1064453125)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_data = list(zip(dim_list, marginal_logprob_list))\n",
    "plot_data.sort(key=lambda x: x[0])\n",
    "dim, marginals = list(zip(*plot_data))\n",
    "\n",
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2c30550e504c7493573db01d3b0492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(dim, marginals)\n",
    "plt.xlabel('Dim')\n",
    "plt.ylabel('log p(x)') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(dim, acc)\n",
    "plt.xlabel('Dim')\n",
    "plt.ylabel('reconstruction acc') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latent_features",
   "language": "python",
   "name": "latent_features"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
