{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_cosmosqa(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            raw_data.append(json.loads(line))\n",
    "\n",
    "    context_lengths = []\n",
    "\n",
    "    for instance in tqdm(raw_data):\n",
    "        context = instance['context']\n",
    "        question = instance['question']\n",
    "        answers = [instance['answer0'], instance['answer1'], instance['answer2'], instance['answer3']]\n",
    "        longest_answer = sorted(answers, key=lambda e: len(e))[-1]\n",
    "\n",
    "        context = \" </s> </s>\".join([context, question, longest_answer])\n",
    "        tokens = context.split(\" \")\n",
    "        if tokenizer:\n",
    "            tokens = tokenizer.tokenize(context)\n",
    "        length = len(tokens)\n",
    "        context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25262/25262 [00:12<00:00, 2105.04it/s]\n",
      "100%|██████████| 1492/1492 [00:00<00:00, 2161.15it/s]\n",
      "100%|██████████| 1493/1493 [00:00<00:00, 2274.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.0 0.0\n",
      "val 0.0 0.0\n",
      "test 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "data_path = '../irt_data/cosmosqa'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"val\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_cosmosqa(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"val\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_boolq(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            raw_data.append(json.loads(line))\n",
    "\n",
    "    context_lengths = []\n",
    "\n",
    "    for instance in tqdm(raw_data):\n",
    "        context = instance['passage']\n",
    "        question = instance['question']\n",
    "\n",
    "        context = \" </s> </s>\".join([context, question])\n",
    "        tokens = context.split(\" \")\n",
    "        if tokenizer:\n",
    "            tokens = tokenizer.tokenize(context)\n",
    "        length = len(tokens)\n",
    "        context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9427/9427 [00:06<00:00, 1347.04it/s]\n",
      "100%|██████████| 1635/1635 [00:00<00:00, 1755.38it/s]\n",
      "100%|██████████| 1635/1635 [00:00<00:00, 1761.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 5.66458046037976 0.1803330858173332\n",
      "val 4.8318042813455655 0.3669724770642202\n",
      "test 5.321100917431193 0.24464831804281345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "data_path = '../irt_data/boolq'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"val\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_boolq(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"val\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_hellaswag(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            raw_data.append(json.loads(line))\n",
    "\n",
    "    context_lengths = []\n",
    "\n",
    "    for instance in tqdm(raw_data):\n",
    "        context = instance['ctx']\n",
    "        question = instance['ctx_b']\n",
    "        longest_answer = sorted(instance['endings'], key=lambda e: len(e))[-1]\n",
    "\n",
    "        context = \" </s> </s>\".join([context, question, longest_answer])\n",
    "        tokens = context.split(\" \")\n",
    "        if tokenizer:\n",
    "            tokens = tokenizer.tokenize(context)\n",
    "        length = len(tokens)\n",
    "        context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39905/39905 [00:17<00:00, 2217.04it/s]\n",
      "100%|██████████| 5021/5021 [00:02<00:00, 2388.43it/s]\n",
      "100%|██████████| 5021/5021 [00:02<00:00, 2400.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.0 0.0\n",
      "val 0.0 0.0\n",
      "test 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "data_path = '../irt_data/hellaswag'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"val\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_hellaswag(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"val\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_mutual(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            raw_data.append(json.loads(line))\n",
    "\n",
    "    context_lengths = []\n",
    "\n",
    "    for instance in tqdm(raw_data):\n",
    "        context = instance['article']\n",
    "        longest_answer = sorted(instance['options'], key=lambda e: len(e))[-1]\n",
    "\n",
    "        context = \" </s> </s>\".join([context, longest_answer])\n",
    "        tokens = context.split(\" \")\n",
    "        if tokenizer:\n",
    "            tokens = tokenizer.tokenize(context)\n",
    "        length = len(tokens)\n",
    "        context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7088/7088 [00:03<00:00, 2248.37it/s]\n",
      "100%|██████████| 443/443 [00:00<00:00, 2458.35it/s]\n",
      "100%|██████████| 443/443 [00:00<00:00, 2413.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 5.431715575620768 0.0\n",
      "dev 3.386004514672686 0.0\n",
      "test 6.772009029345372 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "data_path = '../irt_data/mutual/data/mutual'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"dev\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_mutual(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"dev\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7088/7088 [00:03<00:00, 2066.18it/s]\n",
      "100%|██████████| 443/443 [00:00<00:00, 1425.89it/s]\n",
      "100%|██████████| 443/443 [00:00<00:00, 1955.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 4.500564334085778 0.0\n",
      "dev 3.8374717832957113 0.0\n",
      "test 4.063205417607223 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "data_path = '../irt_data/mutual/data/mutual_plus'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"dev\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_mutual(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"dev\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_mcscript(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            raw_data.append(json.loads(line))\n",
    "\n",
    "    context_lengths = []\n",
    "\n",
    "    for instance in tqdm(raw_data):\n",
    "        context = instance['passage']['text']\n",
    "        questions = instance['passage']['questions']\n",
    "        \n",
    "        for q in questions:\n",
    "            qas = []\n",
    "            for a in q['answers']:\n",
    "                qa = q['question'] + ' ' + a['text']\n",
    "                qas.append(qa)\n",
    "            \n",
    "            longest_answer = sorted(qa, key=lambda e: len(e))[-1]\n",
    "\n",
    "            context = \" </s> </s>\".join([context, longest_answer])\n",
    "            tokens = context.split(\" \")\n",
    "            if tokenizer:\n",
    "                tokens = tokenizer.tokenize(context)\n",
    "            length = len(tokens)\n",
    "            context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:08<00:00, 291.46it/s]\n",
      "100%|██████████| 355/355 [00:01<00:00, 293.76it/s]\n",
      "100%|██████████| 632/632 [00:02<00:00, 282.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 5.862870833626947 0.10570079627933197\n",
      "val 4.603960396039604 0.0\n",
      "test 8.9196675900277 0.08310249307479224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = '../irt_data/mcscript_2.0'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"val\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_mcscript(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"val\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_quail(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            raw_data.append(json.loads(line))\n",
    "\n",
    "    context_lengths = []\n",
    "\n",
    "    for instance in tqdm(raw_data):\n",
    "        context = instance['context']\n",
    "        question = instance['question']\n",
    "        longest_answer = sorted(instance['answers'], key = lambda e: len(e))[-1]\n",
    "\n",
    "        context = \" </s> </s>\".join([context, question, longest_answer])\n",
    "        tokens = context.split(\" \")\n",
    "        if tokenizer:\n",
    "            tokens = tokenizer.tokenize(context)\n",
    "        length = len(tokens)\n",
    "        context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10246/10246 [00:13<00:00, 763.72it/s]\n",
      "100%|██████████| 2164/2164 [00:02<00:00, 776.40it/s]\n",
      "100%|██████████| 555/555 [00:00<00:00, 803.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 95.93987897716183 4.060121022838181\n",
      "val 95.00924214417745 4.990757855822551\n",
      "test 89.90990990990991 10.09009009009009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = '../irt_data/quail'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"val\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_quail(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"val\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_mrqa_nq(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            items = json.loads(line)\n",
    "            if \"header\" in items:\n",
    "                continue\n",
    "            raw_data.append(items)\n",
    "\n",
    "    context_lengths = []\n",
    "\n",
    "    for instance in tqdm(raw_data):\n",
    "        context = instance['context']\n",
    "        question = instance['qas'][0]['question']\n",
    "        answers = instance['qas'][0]['answers']\n",
    "        longest_answer = sorted(answers, key = lambda e: len(e))[-1]\n",
    "\n",
    "        context = \" </s> </s>\".join([context, question, longest_answer])\n",
    "        tokens = context.split(\" \")\n",
    "        if tokenizer:\n",
    "            tokens = tokenizer.tokenize(context)\n",
    "        length = len(tokens)\n",
    "        context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104071/104071 [01:32<00:00, 1130.11it/s]\n",
      "100%|██████████| 6418/6418 [00:05<00:00, 1198.76it/s]\n",
      "100%|██████████| 6418/6418 [00:05<00:00, 1273.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 10.15364510766688 10.608142518088613\n",
      "val 9.909629167965099 12.29354939233406\n",
      "test 10.003116235587411 12.480523527578685\n"
     ]
    }
   ],
   "source": [
    "data_path = '../irt_data/mrqa_natural_questions'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"val\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_mrqa_nq(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"val\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_newsqa(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            items = json.loads(line)\n",
    "            if \"header\" in items:\n",
    "                continue\n",
    "            raw_data.append(items)\n",
    "\n",
    "    context_lengths = []\n",
    "\n",
    "    for instance in tqdm(raw_data):\n",
    "        context = instance['text']\n",
    "        qas = instance['qas']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            answer = context[qa['answer']['s']:qa['answer']['e']+1]\n",
    "\n",
    "            context = \" </s> </s>\".join([context, question, answer])\n",
    "            tokens = context.split(\" \")\n",
    "            if tokenizer:\n",
    "                tokens = tokenizer.tokenize(context)\n",
    "            length = len(tokens)\n",
    "            context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11469/11469 [03:19<00:00, 57.37it/s]\n",
      "100%|██████████| 638/638 [00:10<00:00, 59.27it/s]\n",
      "100%|██████████| 637/637 [00:10<00:00, 60.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 15.84604534531397 83.28022150245533\n",
      "val 16.624453142988717 82.5926778724384\n",
      "test 16.538551129746097 82.64616818075937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = '../irt_data/newsqa'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"val\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_newsqa(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"val\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76568, 4343, 4293]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_quoref(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            items = json.loads(line)\n",
    "            if \"header\" in items:\n",
    "                continue\n",
    "            raw_data.append(items)\n",
    "\n",
    "    context_lengths = []\n",
    "\n",
    "    for instance in tqdm(raw_data):\n",
    "        context = instance['context']\n",
    "        question = instance['question']\n",
    "        answers = []\n",
    "        for a in instance['answers']['text']:\n",
    "            answers.append(a)\n",
    "        \n",
    "        longest_answer = sorted(answers, key=lambda e: len(e))[-1]\n",
    "        context = \" </s> </s>\".join([context, question, longest_answer])\n",
    "        tokens = context.split(\" \")\n",
    "        if tokenizer:\n",
    "            tokens = tokenizer.tokenize(context)\n",
    "        length = len(tokens)\n",
    "        context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19399/19399 [00:25<00:00, 746.91it/s]\n",
      "100%|██████████| 1209/1209 [00:01<00:00, 760.14it/s]\n",
      "100%|██████████| 1209/1209 [00:01<00:00, 806.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 62.91045930202588 35.23377493685241\n",
      "val 65.34325889164599 33.41604631927213\n",
      "test 66.3358147229115 32.09263854425145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = '../irt_data/quoref'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"val\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \".jsonl\")\n",
    "    context_lengths = get_context_lengths_quoref(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"val\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../irt_data/squad_v2/dev-v2.0.json'\n",
    "f = open(data_path)\n",
    "json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qas': [{'question': 'In what country is Normandy located?',\n",
       "   'id': '56ddde6b9a695914005b9628',\n",
       "   'answers': [{'text': 'France', 'answer_start': 159},\n",
       "    {'text': 'France', 'answer_start': 159},\n",
       "    {'text': 'France', 'answer_start': 159},\n",
       "    {'text': 'France', 'answer_start': 159}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'When were the Normans in Normandy?',\n",
       "   'id': '56ddde6b9a695914005b9629',\n",
       "   'answers': [{'text': '10th and 11th centuries', 'answer_start': 94},\n",
       "    {'text': 'in the 10th and 11th centuries', 'answer_start': 87},\n",
       "    {'text': '10th and 11th centuries', 'answer_start': 94},\n",
       "    {'text': '10th and 11th centuries', 'answer_start': 94}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'From which countries did the Norse originate?',\n",
       "   'id': '56ddde6b9a695914005b962a',\n",
       "   'answers': [{'text': 'Denmark, Iceland and Norway', 'answer_start': 256},\n",
       "    {'text': 'Denmark, Iceland and Norway', 'answer_start': 256},\n",
       "    {'text': 'Denmark, Iceland and Norway', 'answer_start': 256},\n",
       "    {'text': 'Denmark, Iceland and Norway', 'answer_start': 256}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'Who was the Norse leader?',\n",
       "   'id': '56ddde6b9a695914005b962b',\n",
       "   'answers': [{'text': 'Rollo', 'answer_start': 308},\n",
       "    {'text': 'Rollo', 'answer_start': 308},\n",
       "    {'text': 'Rollo', 'answer_start': 308},\n",
       "    {'text': 'Rollo', 'answer_start': 308}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What century did the Normans first gain their separate identity?',\n",
       "   'id': '56ddde6b9a695914005b962c',\n",
       "   'answers': [{'text': '10th century', 'answer_start': 671},\n",
       "    {'text': 'the first half of the 10th century', 'answer_start': 649},\n",
       "    {'text': '10th', 'answer_start': 671},\n",
       "    {'text': '10th', 'answer_start': 671}],\n",
       "   'is_impossible': False},\n",
       "  {'plausible_answers': [{'text': 'Normans', 'answer_start': 4}],\n",
       "   'question': \"Who gave their name to Normandy in the 1000's and 1100's\",\n",
       "   'id': '5ad39d53604f3c001a3fe8d1',\n",
       "   'answers': [],\n",
       "   'is_impossible': True},\n",
       "  {'plausible_answers': [{'text': 'Normandy', 'answer_start': 137}],\n",
       "   'question': 'What is France a region of?',\n",
       "   'id': '5ad39d53604f3c001a3fe8d2',\n",
       "   'answers': [],\n",
       "   'is_impossible': True},\n",
       "  {'plausible_answers': [{'text': 'Rollo', 'answer_start': 308}],\n",
       "   'question': 'Who did King Charles III swear fealty to?',\n",
       "   'id': '5ad39d53604f3c001a3fe8d3',\n",
       "   'answers': [],\n",
       "   'is_impossible': True},\n",
       "  {'plausible_answers': [{'text': '10th century', 'answer_start': 671}],\n",
       "   'question': 'When did the Frankish identity emerge?',\n",
       "   'id': '5ad39d53604f3c001a3fe8d4',\n",
       "   'answers': [],\n",
       "   'is_impossible': True}],\n",
       " 'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_lengths_squad(raw_file_path, tokenizer = None):\n",
    "    raw_data = []\n",
    "    with open(raw_file_path, \"r\") as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    context_lengths = []\n",
    "    for data in tqdm(raw_data['data']):\n",
    "        for instance in data['paragraphs']:\n",
    "            context = instance['context']\n",
    "            questions = instance['qas']\n",
    "            for q in questions:\n",
    "                question = q['question']\n",
    "                if q['answers']:\n",
    "                    answers = [x['text'] for x in q['answers']]\n",
    "                else:\n",
    "                    answers = [x['text'] for x in q['plausible_answers']]\n",
    "\n",
    "                if answers:\n",
    "                    longest_answer = sorted(answers, key=lambda e: len(e))[-1]\n",
    "                else:\n",
    "                    longest_answer = \"\"\n",
    "                context = \" </s> </s>\".join([context, question, longest_answer])\n",
    "                tokens = context.split(\" \")\n",
    "                if tokenizer:\n",
    "                    tokens = tokenizer.tokenize(context)\n",
    "                length = len(tokens)\n",
    "                context_lengths.append(length)\n",
    "    return context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [02:05<00:00,  3.53it/s]\n",
      "100%|██████████| 17/17 [00:06<00:00,  2.71it/s]\n",
      "100%|██████████| 17/17 [00:06<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 47.641556488309455 2.6557907902915154\n",
      "dev 58.37885462555066 7.365638766519824\n",
      "test 58.37885462555066 7.365638766519824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = '../irt_data/squad_v2'\n",
    "histograms = []\n",
    "num_examples = []\n",
    "for phase in [\"train\", \"dev\", \"test\"]:\n",
    "    path = os.path.join(data_path, phase + \"-v2.0.json\")\n",
    "    context_lengths = get_context_lengths_squad(path, tokenizer)\n",
    "    hist, _ = np.histogram(context_lengths, bins=[0, 129, 257, 513, 10000])\n",
    "    histograms.append(hist)\n",
    "    num_examples.append(len(context_lengths))\n",
    "\n",
    "for phase, hist, total in zip([\"train\", \"dev\", \"test\"], histograms, num_examples):\n",
    "    print(phase, hist[2] * 100 / total, hist[3] * 100 / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[130319, 5675, 5675]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
