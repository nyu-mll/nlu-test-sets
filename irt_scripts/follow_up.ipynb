{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle5 as pickle\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy\n",
    "import torch\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "import pyro\n",
    "import pyro.infer\n",
    "import pyro.infer.mcmc\n",
    "import pyro.distributions as dist\n",
    "import torch.distributions.constraints as constraints\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1.+torch.exp(-x))\n",
    "\n",
    "def icc_best_deriv(alpha, beta, theta, model_names, gamma=None, col='mean', target=None):\n",
    "    '''\n",
    "    Method to calculate the locally estimated headroom (LEH) score, defined as\n",
    "    the derivative of the item characteristic curve w.r.t. the best performing model.\n",
    "    \n",
    "    Args:\n",
    "        alpha:       DataFrame of discrimination parameter statistics for each item.\n",
    "        beta:        DataFrame of difficulty parameter statistics for each item.\n",
    "        theta:       DataFrame of ability parameter statistics for each responder.\n",
    "        model_names: List of responder names.\n",
    "        gamma:       DataFrame of guessing parameter statistics for each item.\n",
    "        col:         DataFrame column name to use for calculating LEH scores.\n",
    "    \n",
    "    Returns:\n",
    "        scores:      LEH scores for each item.    \n",
    "    '''\n",
    "    if target:\n",
    "        idx = model_names.index(target)\n",
    "        best_value = theta[col].iloc[idx]\n",
    "        print('Local Grad for Target Model')\n",
    "        print(f'Target model: {target}\\n{best_value}')\n",
    "    else:\n",
    "        best_idx, best_value = theta[col].argmax(), theta[col].max()\n",
    "        print(f'Best model: {model_names[best_idx]}\\n{best_value}')\n",
    "    \n",
    "    a, b = torch.tensor(alpha[col].values), torch.tensor(beta[col].values)\n",
    "    \n",
    "    logits = (a*(best_value-b))\n",
    "    sigmoids = sigmoid(logits)\n",
    "    scores = sigmoids*(1.-sigmoids)*a\n",
    "    \n",
    "    print(f'No gamma: {scores.mean()}')\n",
    "    if not gamma is None:\n",
    "        g = torch.tensor(gamma[col].apply(lambda x: x.item()).values)\n",
    "        scores = (1.-g)*scores\n",
    "        print(f'With gamma: {scores.mean()}')\n",
    "    \n",
    "    return scores      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_guide(alpha_dist, theta_dist, alpha_transform, theta_transform):\n",
    "    model = lambda obs: irt_model(obs, alpha_dist, theta_dist, alpha_transform = alpha_transform, theta_transform = theta_transform)\n",
    "    guide = lambda obs: vi_posterior(obs, alpha_dist, theta_dist)\n",
    "    \n",
    "    return model, guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_accuracies(data, verbose = False, get_cols = False):\n",
    "    '''\n",
    "    Method to reformat `data` and calculate item and responder accuracies.\n",
    "    \n",
    "    Args:\n",
    "        data:                DataFrame of item responses.\n",
    "        verbose:             Boolean value of whether to print statements.\n",
    "        get_cols:            Boolean value of whether to return original column\n",
    "                             values of `data`.\n",
    "        \n",
    "    Returns:\n",
    "        new_data:            Reformatted `data`, dropping first column.\n",
    "        accuracies:          Accuracy for each responder across examples.\n",
    "        example_accuracies:  Accuracy for each example across responders.\n",
    "        data.columns.values: Returns only if `get_cols` is True. Original column\n",
    "                             values of `data`.\n",
    "    '''\n",
    "    new_data = numpy.array(data)\n",
    "    new_data = new_data[:,1:]\n",
    "    \n",
    "    model_names = dict(data['userid'])\n",
    "    accuracies = new_data.mean(-1)\n",
    "    example_accuracies = new_data.mean(0)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n'.join([f'{name}: {acc}' for name, acc in zip(model_names.values(),accuracies)]))\n",
    "    \n",
    "    if get_cols:\n",
    "        return new_data, accuracies, example_accuracies, data.columns.values\n",
    "    else:\n",
    "        return new_data, accuracies, example_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_CI(params, p=0.95, dist='normal'):\n",
    "    '''\n",
    "    Method to calculate lower and upper quantiles defined by `p`, mean, and variance of `param`\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary of distribution parameters for each item keyed according to the \n",
    "                parametric distribution defined by `dist`.\n",
    "        p:      Percent of distribution covered by the lower and upper interval values for each\n",
    "                parameter.\n",
    "        dist:   Name of parametric distribution\n",
    "    \n",
    "    Returns:\n",
    "        return: {\n",
    "            'lower': Lower interval values of each parameter,\n",
    "            'upper': Upper interval values of each parameter,\n",
    "            'mean' : Mean of each parameter,\n",
    "            'var'  : Variance of each parameter\n",
    "        }\n",
    "    '''\n",
    "    stats = {}\n",
    "    if dist == 'normal':\n",
    "        L,U = scipy.stats.norm.interval(p,loc=params['mu'], scale=torch.exp(params['logstd']))\n",
    "        M,V = scipy.stats.norm.stats(loc=params['mu'], scale=torch.exp(params['logstd']))\n",
    "    elif dist == 'log-normal':\n",
    "        L,U = scipy.stats.lognorm.interval(p, s=torch.exp(params['logstd']), scale=torch.exp(params['mu']))\n",
    "        M,V = scipy.stats.lognorm.stats(s=torch.exp(params['logstd']), scale=torch.exp(params['mu']))\n",
    "    elif dist == 'beta':\n",
    "        L,U = scipy.stats.beta.interval(p,a=params['alpha'], b=params['beta'])\n",
    "        M,V = scipy.stats.beta.stats(a=params['alpha'], b=params['beta'])\n",
    "    else:\n",
    "        raise TypeError(f'Distribution type {dist} not supported.')\n",
    "    \n",
    "    return {\n",
    "        'lower':[L],\n",
    "        'upper':[U],\n",
    "        'mean':[M],\n",
    "        'var':[V],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_stats(exp_dir, alpha_dist, theta_dist, transforms, p = 0.95):\n",
    "    '''\n",
    "    Method to return plotting statistics for 3 parameter IRT model parameters.\n",
    "    \n",
    "    Args:\n",
    "        exp_dir:          Path to 3 parameter IRT parameters and responses.\n",
    "        alpha_dist:       Name of the item discrimination [a] distribution.\n",
    "        theta_dist:       Name of the responder ability [t] distribution.\n",
    "        transforms:       Dictionary of transformations to apply to each parameter type\n",
    "                          where keys are parameter names and values are functions.\n",
    "        p:                Percent of distribution covered by the lower and upper interval \n",
    "                          values for each parameter.\n",
    "    \n",
    "    Returns:\n",
    "        param_plot_stats: Dictionary of parameter plot statistics where keys are parameter\n",
    "                          names and values are plot statistics dictionaries as defined by\n",
    "                          get_stats_CI().\n",
    "    '''\n",
    "    param_dists = {\n",
    "        'a':alpha_dist,\n",
    "        'b':'normal',\n",
    "        'g':'normal',\n",
    "        't':theta_dist,\n",
    "    }\n",
    "\n",
    "    dist_params = {\n",
    "        'normal':['mu', 'logstd'],\n",
    "        'log-normal':['mu', 'logstd'],\n",
    "        'beta':['alpha', 'beta'],\n",
    "    }\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "    pyro.get_param_store().load(os.path.join(exp_dir, 'params.p'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pyro_param_dict = dict(pyro.get_param_store().named_parameters())\n",
    "    \n",
    "    # get stats for plotting\n",
    "    param_plot_stats = {}\n",
    "\n",
    "    for param, param_dist in param_dists.items():\n",
    "        temp_params = dist_params[param_dist]\n",
    "\n",
    "        for idx, (p1_orig, p2_orig) in enumerate(zip(pyro_param_dict[f'{param} {temp_params[0]}'], pyro_param_dict[f'{param} {temp_params[1]}'])):\n",
    "            p1, p2 = p1_orig.detach(), p2_orig.detach()\n",
    "            \n",
    "            temp_stats_df = pd.DataFrame.from_dict(\n",
    "                get_stats_CI(\n",
    "                    params = {\n",
    "                        temp_params[0]:p1,\n",
    "                        temp_params[1]:p2,\n",
    "                    },\n",
    "                    p=p,\n",
    "                    dist = param_dist,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            temp_stats_df = temp_stats_df.applymap(transforms[param])\n",
    "        \n",
    "            if idx == 0:\n",
    "                param_plot_stats[param] = temp_stats_df\n",
    "            else:\n",
    "                param_plot_stats[param] = param_plot_stats[param].append(temp_stats_df, ignore_index = True)\n",
    "    \n",
    "    return param_plot_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_mult(df1, df2):\n",
    "    newdf = copy.deepcopy(df2)\n",
    "    \n",
    "    for idx, row in df1.iterrows():\n",
    "        if numpy.sign(row['mean']) < 0:\n",
    "            newdf.loc[idx,'mean'] = -1*newdf.loc[idx,'mean']\n",
    "            newdf.loc[idx,'lower'] = -1*newdf.loc[idx,'upper']\n",
    "            newdf.loc[idx,'upper'] = -1*newdf.loc[idx,'lower']\n",
    "    \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_by_set(diffs, item_ids):\n",
    "    diff_by_set = {}\n",
    "    id_split = '_'\n",
    "\n",
    "    max_diff = -1e6\n",
    "    min_diff = 1e6\n",
    "    \n",
    "    for idx, diff in enumerate(diffs):\n",
    "        set_name = item_ids[idx].split(id_split)[0]\n",
    "\n",
    "        if set_name in diff_by_set.keys():\n",
    "            diff_by_set[set_name].append(diff)\n",
    "        else:\n",
    "            diff_by_set[set_name] = [diff]\n",
    "            \n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            \n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "    \n",
    "    return diff_by_set, min_diff, max_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variational_irt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=\"boolq,cb,commonsenseqa,copa,cosmosqa,hellaswag,adversarial-nli,rte,snli,wic,qamr,arct,mcscript,mctaco,mutual,mutual-plus,quoref,socialiqa,squad-v2,wsc,mnli,mrqa-nq,newsqa,abductive-nli,arc-easy,arc-challenge,piqa,quail,winogrande,anli\"\n",
    "data_names, responses, n_items = get_files(\n",
    "    os.path.join(repo, 'data'),\n",
    "    \"csv\",\n",
    "    set(datasets.split(','))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metadata = pd.read_csv('task_metadata.csv')\n",
    "task_metadata.set_index(\"jiant_name\", inplace=True)\n",
    "task_list = [x for x in task_metadata.index if x in data_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "task_name = []\n",
    "task_format = []\n",
    "\n",
    "for tname, size in zip(data_names, n_items):\n",
    "    name = task_metadata.loc[tname]['taskname']\n",
    "    total += size\n",
    "    task_name += [name for _ in range(size)]\n",
    "    task_format += [task_metadata.loc[tname]['format'] for _ in range(size)]\n",
    "    \n",
    "task_name = pd.DataFrame(task_name, columns=['task_name'])\n",
    "task_format = pd.DataFrame(task_format, columns=['format'])\n",
    "task_name_format = pd.concat([task_name, task_format], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Params and Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = os.path.join(repo, 'params', f'alpha-lognormal-identity_theta-normal-identity_nosubsample_1.00_0.30')\n",
    "p = 0.95\n",
    "\n",
    "with open(os.path.join(exp_dir, 'responses.p'), 'rb') as f:\n",
    "    combined_responses = pickle.load(f).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-large_best\n",
      "==========================================================================================\n",
      "Overall acc: 0.7421\n",
      "abductive_nli acc: 0.8564\n",
      "adversarial_nli acc: 0.4938\n",
      "arc_challenge acc: 0.3319\n",
      "arc_easy acc: 0.6296\n",
      "arct acc: 0.8604\n",
      "boolq acc: 0.8367\n",
      "cb acc: 0.8571\n",
      "commonsenseqa acc: 0.6759\n",
      "copa acc: 0.8400\n",
      "cosmosqa acc: 0.7984\n",
      "hellaswag acc: 0.8417\n",
      "mcscript acc: 0.9183\n",
      "mctaco acc: 0.5360\n",
      "mnli acc: 0.8991\n",
      "mrqa_natural_questions acc: 0.6941\n",
      "mutual_plus acc: 0.7314\n",
      "mutual acc: 0.8668\n",
      "newsqa acc: 0.5542\n",
      "piqa acc: 0.7617\n",
      "qamr acc: 0.7303\n",
      "quail acc: 0.6691\n",
      "quoref acc: 0.8023\n",
      "rte acc: 0.8345\n",
      "snli acc: 0.9203\n",
      "socialiqa acc: 0.7738\n",
      "squad_v2 acc: 0.4301\n",
      "wic acc: 0.7085\n",
      "winogrande acc: 0.7697\n",
      "wsc acc: 0.6154\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy of roberta-large models\n",
    "\n",
    "extractmodel = 'roberta-large_best'\n",
    "tie_break = 0\n",
    "\n",
    "acc_by_dataset = {}\n",
    "\n",
    "roberta_rp = combined_responses.loc[combined_responses['userid']==extractmodel, :]\n",
    "if roberta_rp.shape[0] > 1:\n",
    "    roberta_rp = roberta_rp.iloc[tie_break, :]\n",
    "\n",
    "cols = combined_responses.columns.values\n",
    "\n",
    "for item in cols[1:]:\n",
    "    data_name = '_'.join(item.split('_')[:-1])\n",
    "    resp = roberta_rp[item].item()\n",
    "    \n",
    "    if data_name in acc_by_dataset:\n",
    "        acc_by_dataset[data_name]['correct'] += resp\n",
    "        acc_by_dataset[data_name]['total'] += 1\n",
    "    else:\n",
    "        acc_by_dataset[data_name] = {'correct': resp, 'total': 1}\n",
    "\n",
    "print(extractmodel)\n",
    "print('='*90)\n",
    "print(f'Overall acc: {roberta_rp.iloc[0, 1:].sum()/(roberta_rp.shape[1]-1):.4f}')        \n",
    "\n",
    "for data_name, acc_dict in acc_by_dataset.items():\n",
    "    print(f'{data_name} acc: {acc_dict[\"correct\"]/acc_dict[\"total\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to False if run for the first time\n",
    "# note that this will take sometimes to run if the datasets are big\n",
    "load_from_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution and transformation\n",
    "alpha_dist = 'log-normal'\n",
    "alpha_transf = 'standard'\n",
    "theta_dist = 'normal'\n",
    "theta_transf = 'standard'\n",
    "\n",
    "exp_dir = os.path.join(repo, 'params', f'alpha-lognormal-identity_theta-normal-identity_nosubsample_1.00_0.30')\n",
    "p = 0.95\n",
    "\n",
    "with open(os.path.join(exp_dir, 'responses.p'), 'rb') as f:\n",
    "    combined_responses = pickle.load(f).reset_index()\n",
    "data, accuracies, example_accuracies = get_data_accuracies(combined_responses)\n",
    "column_names = combined_responses.columns[1:]\n",
    "select_ts = {\n",
    "    'standard':lambda x:x,\n",
    "    'positive':lambda x:torch.log(1+torch.exp(torch.tensor(x))),\n",
    "    'sigmoid':lambda x:sigmoid(torch.tensor(x)),\n",
    "}\n",
    "\n",
    "transforms = {\n",
    "    'a':select_ts[alpha_transf],\n",
    "    'b':select_ts['standard'],\n",
    "    'g':select_ts['sigmoid'],\n",
    "    't':select_ts[theta_transf],\n",
    "}\n",
    "\n",
    "if load_from_cache:\n",
    "    param_plot_stats = {}\n",
    "\n",
    "    for key in transforms.keys():\n",
    "        with open(os.path.join('plot_stats_pickles', f'{key}.p'), 'rb') as f:\n",
    "            param_plot_stats[key] = pickle.load(f)\n",
    "else:\n",
    "    param_plot_stats = get_plot_stats(\n",
    "        exp_dir,\n",
    "        alpha_dist,\n",
    "        theta_dist,\n",
    "        transforms,\n",
    "        p = 0.95\n",
    "    )\n",
    "    \n",
    "    os.makedirs('plot_stats_pickles', exist_ok=True)\n",
    "    for key, value in param_plot_stats.items():\n",
    "        with open(os.path.join('plot_stats_pickles', f'{key}.p'), 'wb') as f:\n",
    "            pickle.dump(value, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>abductive_nli_0</th>\n",
       "      <th>abductive_nli_1</th>\n",
       "      <th>abductive_nli_2</th>\n",
       "      <th>abductive_nli_3</th>\n",
       "      <th>abductive_nli_4</th>\n",
       "      <th>abductive_nli_5</th>\n",
       "      <th>abductive_nli_6</th>\n",
       "      <th>abductive_nli_7</th>\n",
       "      <th>abductive_nli_8</th>\n",
       "      <th>...</th>\n",
       "      <th>wsc_42</th>\n",
       "      <th>wsc_43</th>\n",
       "      <th>wsc_44</th>\n",
       "      <th>wsc_45</th>\n",
       "      <th>wsc_46</th>\n",
       "      <th>wsc_47</th>\n",
       "      <th>wsc_48</th>\n",
       "      <th>wsc_49</th>\n",
       "      <th>wsc_50</th>\n",
       "      <th>wsc_51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-base-10M-1_best</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-10M-1_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roberta-base-10M-1_25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roberta-base-10M-1_50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base-10M-1_10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>xlm-roberta-large_best</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>xlm-roberta-large_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>xlm-roberta-large_25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>xlm-roberta-large_50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>xlm-roberta-large_10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 82234 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     userid  abductive_nli_0  abductive_nli_1  \\\n",
       "0   roberta-base-10M-1_best                1                0   \n",
       "1      roberta-base-10M-1_1                0                0   \n",
       "2     roberta-base-10M-1_25                1                1   \n",
       "3     roberta-base-10M-1_50                1                0   \n",
       "4     roberta-base-10M-1_10                1                0   \n",
       "..                      ...              ...              ...   \n",
       "85   xlm-roberta-large_best                1                1   \n",
       "86      xlm-roberta-large_1                1                1   \n",
       "87     xlm-roberta-large_25                1                1   \n",
       "88     xlm-roberta-large_50                1                1   \n",
       "89     xlm-roberta-large_10                0                1   \n",
       "\n",
       "    abductive_nli_2  abductive_nli_3  abductive_nli_4  abductive_nli_5  \\\n",
       "0                 1                1                1                1   \n",
       "1                 0                1                1                0   \n",
       "2                 1                1                1                1   \n",
       "3                 1                1                1                1   \n",
       "4                 1                1                1                1   \n",
       "..              ...              ...              ...              ...   \n",
       "85                0                1                1                1   \n",
       "86                0                1                1                0   \n",
       "87                0                1                1                1   \n",
       "88                1                1                1                1   \n",
       "89                0                1                1                0   \n",
       "\n",
       "    abductive_nli_6  abductive_nli_7  abductive_nli_8  ...  wsc_42  wsc_43  \\\n",
       "0                 1                0                1  ...       1       0   \n",
       "1                 1                0                1  ...       1       0   \n",
       "2                 1                1                1  ...       0       0   \n",
       "3                 1                0                1  ...       1       0   \n",
       "4                 1                0                1  ...       1       0   \n",
       "..              ...              ...              ...  ...     ...     ...   \n",
       "85                1                1                1  ...       1       1   \n",
       "86                1                0                1  ...       0       0   \n",
       "87                0                1                1  ...       1       1   \n",
       "88                1                1                1  ...       1       1   \n",
       "89                0                1                1  ...       0       0   \n",
       "\n",
       "    wsc_44  wsc_45  wsc_46  wsc_47  wsc_48  wsc_49  wsc_50  wsc_51  \n",
       "0        1       1       0       1       0       1       0       1  \n",
       "1        1       0       0       1       0       0       0       1  \n",
       "2        0       1       0       1       1       1       1       1  \n",
       "3        0       1       0       1       1       1       0       1  \n",
       "4        0       1       0       1       1       1       0       1  \n",
       "..     ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "85       0       1       0       1       1       1       0       0  \n",
       "86       1       0       1       0       0       0       1       1  \n",
       "87       0       1       0       1       1       1       0       0  \n",
       "88       0       1       0       1       1       1       0       0  \n",
       "89       1       0       1       0       0       0       1       1  \n",
       "\n",
       "[90 rows x 82234 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "model_levels = []\n",
    "for m in combined_responses['userid']:\n",
    "    mname = m.split('_')[0]\n",
    "    mlevel = m.split('_')[-1]\n",
    "    if mname.endswith('-1') or mname.endswith('-2') or mname.endswith('-3'):\n",
    "        mname = mname[:-2]\n",
    "    model_names.append(mname)\n",
    "    \n",
    "    mlevel_append = '' if mlevel == 'best' else r'%'\n",
    "    model_levels.append(mlevel+mlevel_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "b 36860\n",
      "g 0\n",
      "t 49\n"
     ]
    }
   ],
   "source": [
    "#  we will only use log mean for discriminative parameter\n",
    "for param_key, param_stat in param_plot_stats.items():\n",
    "    param_stat['log_mean'] = numpy.log(param_stat['mean'])\n",
    "    print(param_key, param_stat['log_mean'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_a = pd.concat([param_plot_stats['a'], task_name_format], axis=1)\n",
    "param_b = pd.concat([param_plot_stats['b'], task_name_format], axis=1)\n",
    "\n",
    "task_order = [task_metadata.loc[x]['taskname'] for x in task_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: albert-xxlarge-v2\n",
      "1.582378625869751\n",
      "No gamma: 0.14226206233809996\n",
      "With gamma: 0.11365353401378117\n",
      "           mean\n",
      "0      0.193985\n",
      "1      0.178925\n",
      "2      0.057262\n",
      "3      0.014374\n",
      "4      0.105861\n",
      "...         ...\n",
      "82228  0.059020\n",
      "82229  0.123432\n",
      "82230  0.083337\n",
      "82231  0.107942\n",
      "82232  0.082005\n",
      "\n",
      "[82233 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "leh_scores = icc_best_deriv(\n",
    "    param_plot_stats['a'],\n",
    "    param_plot_stats['b'],\n",
    "    param_plot_stats['t'],\n",
    "    model_names,\n",
    "    gamma = param_plot_stats['g'],\n",
    ")\n",
    "\n",
    "leh_scores_plot = pd.DataFrame(pd.Series(leh_scores), columns = ['mean'])\n",
    "print(leh_scores_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leh_scores_plot = pd.concat([leh_scores_plot, task_name_format], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metadata.set_index(\"taskname\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow-ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## % all incorrect/correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcent_correct = pd.DataFrame(\n",
    "    combined_responses.iloc[:,1:].sum(axis=0)/combined_responses.shape[0], columns=['pcent_correct']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcent_correct['all_correct'] = pcent_correct['pcent_correct'].eq(1)\n",
    "pcent_correct['all_wrong'] = pcent_correct['pcent_correct'].eq(0)\n",
    "pcent_correct['all_either'] = pcent_correct['all_correct'] + pcent_correct['all_wrong']\n",
    "pcent_correct['dataset'] = list(pd.Series(pcent_correct.index.values).apply(lambda x: '_'.join(x.split('_')[:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pcent_correct</th>\n",
       "      <th>all_correct</th>\n",
       "      <th>all_wrong</th>\n",
       "      <th>all_either</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abductive_nli_0</th>\n",
       "      <td>0.466667</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abductive_nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abductive_nli_1</th>\n",
       "      <td>0.388889</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abductive_nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abductive_nli_2</th>\n",
       "      <td>0.744444</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abductive_nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abductive_nli_3</th>\n",
       "      <td>0.844444</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abductive_nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abductive_nli_4</th>\n",
       "      <td>0.644444</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>abductive_nli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wsc_47</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>wsc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wsc_48</th>\n",
       "      <td>0.544444</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>wsc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wsc_49</th>\n",
       "      <td>0.633333</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>wsc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wsc_50</th>\n",
       "      <td>0.311111</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>wsc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wsc_51</th>\n",
       "      <td>0.688889</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>wsc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82233 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pcent_correct  all_correct  all_wrong  all_either  \\\n",
       "abductive_nli_0       0.466667        False      False       False   \n",
       "abductive_nli_1       0.388889        False      False       False   \n",
       "abductive_nli_2       0.744444        False      False       False   \n",
       "abductive_nli_3       0.844444        False      False       False   \n",
       "abductive_nli_4       0.644444        False      False       False   \n",
       "...                        ...          ...        ...         ...   \n",
       "wsc_47                0.777778        False      False       False   \n",
       "wsc_48                0.544444        False      False       False   \n",
       "wsc_49                0.633333        False      False       False   \n",
       "wsc_50                0.311111        False      False       False   \n",
       "wsc_51                0.688889        False      False       False   \n",
       "\n",
       "                       dataset  \n",
       "abductive_nli_0  abductive_nli  \n",
       "abductive_nli_1  abductive_nli  \n",
       "abductive_nli_2  abductive_nli  \n",
       "abductive_nli_3  abductive_nli  \n",
       "abductive_nli_4  abductive_nli  \n",
       "...                        ...  \n",
       "wsc_47                     wsc  \n",
       "wsc_48                     wsc  \n",
       "wsc_49                     wsc  \n",
       "wsc_50                     wsc  \n",
       "wsc_51                     wsc  \n",
       "\n",
       "[82233 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude lower       0.874477\n",
      "upper       2.108341\n",
      "mean        1.387854\n",
      "var         0.109594\n",
      "log_mean    0.298865\n",
      "dtype: float64\n",
      "\n",
      "only lower       0.992734\n",
      "upper       2.361330\n",
      "mean        1.564777\n",
      "var         0.128509\n",
      "log_mean    0.436691\n",
      "dtype: float64\n",
      "\n",
      "only correct lower       0.622756\n",
      "upper       1.929687\n",
      "mean        1.138003\n",
      "var         0.120141\n",
      "log_mean    0.122375\n",
      "dtype: float64\n",
      "\n",
      "only wrong lower       1.049897\n",
      "upper       2.428020\n",
      "mean        1.630715\n",
      "var         0.129802\n",
      "log_mean    0.485254\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('exclude', param_plot_stats['a'].loc[~np.array(pcent_correct['all_either']),:].mean())\n",
    "print('')\n",
    "print('only', param_plot_stats['a'].loc[np.array(pcent_correct['all_either']),:].mean())\n",
    "print('')\n",
    "print('only correct', param_plot_stats['a'].loc[np.array(pcent_correct['all_correct']),:].mean())\n",
    "print('')\n",
    "print('only wrong', param_plot_stats['a'].loc[np.array(pcent_correct['all_wrong']),:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude lower       0.148670\n",
      "upper       0.392903\n",
      "mean        0.249761\n",
      "var         0.549226\n",
      "log_mean   -1.675285\n",
      "dtype: float64\n",
      "\n",
      "only lower       0.114596\n",
      "upper       0.205577\n",
      "mean        0.152623\n",
      "var         0.563053\n",
      "log_mean   -2.908818\n",
      "dtype: float64\n",
      "\n",
      "only correct lower       0.765916\n",
      "upper       0.968994\n",
      "mean        0.911116\n",
      "var         0.584755\n",
      "log_mean   -0.093166\n",
      "dtype: float64\n",
      "\n",
      "only wrong lower       0.013965\n",
      "upper       0.087626\n",
      "mean        0.035433\n",
      "var         0.559700\n",
      "log_mean   -3.343847\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('exclude', param_plot_stats['g'].loc[~np.array(pcent_correct['all_either']),:].mean())\n",
    "print('')\n",
    "print('only', param_plot_stats['g'].loc[np.array(pcent_correct['all_either']),:].mean())\n",
    "print('')\n",
    "print('only correct', param_plot_stats['g'].loc[np.array(pcent_correct['all_correct']),:].mean())\n",
    "print('')\n",
    "print('only wrong', param_plot_stats['g'].loc[np.array(pcent_correct['all_wrong']),:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([pcent_correct.reset_index(), task_name_format], axis=1)\n",
    "\n",
    "dataset2format = {'all':'all'}\n",
    "format2dataset = {'all':'all'}\n",
    "\n",
    "for dset in temp['dataset'].unique():\n",
    "    fmat = temp.loc[temp['dataset'] == dset, 'format'].unique()[0]\n",
    "    dataset2format[dset] = fmat\n",
    "    format2dataset[fmat] = dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversarial_nli\n"
     ]
    }
   ],
   "source": [
    "item_excludes = {}\n",
    "\n",
    "for dataset in pcent_correct['dataset'].unique():\n",
    "    temp = pcent_correct.loc[pcent_correct['dataset'] == dataset, :]\n",
    "    \n",
    "    try:\n",
    "        assert np.not_equal(temp.index.values, np.array(sorted(temp.index.values, key=lambda x: int(x.split('_')[-1])))).sum() == 0, dataset\n",
    "    except:\n",
    "        print(dataset)\n",
    "        pass\n",
    "    \n",
    "    item_excludes[dataset] = temp['all_either']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keeps = ['all_correct', 'all_wrong', 'all_either', 'dataset']\n",
    "grouped_pcent_correct = pcent_correct[keeps].groupby(by='dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all_pcent = {}\n",
    "combined_all_count = {}\n",
    "\n",
    "for col in ['all_correct', 'all_wrong', 'all_either']:\n",
    "    combined_all_count[col] = pcent_correct[col].sum()\n",
    "    combined_all_pcent[col] = combined_all_count[col]/combined_responses.shape[1]\n",
    "    \n",
    "combined_all_count['total_count'] = combined_responses.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_pcent_either = grouped_pcent_correct.sum()/grouped_pcent_correct.count()\n",
    "summary_pcent = grouped_pcent_either.append(pd.DataFrame.from_dict({'all':combined_all_pcent}, orient='index'))\n",
    "\n",
    "grouped_count_either = grouped_pcent_correct.sum()\n",
    "grouped_count_either['total_count'] = grouped_pcent_correct.count()['all_either']\n",
    "summary_count = grouped_count_either.append(pd.DataFrame.from_dict({'all':combined_all_count}, orient='index'))\n",
    "\n",
    "\n",
    "summary_pcent['format'] = list(pd.Series(summary_pcent.index.values).apply(lambda x: dataset2format[x]))\n",
    "summary_count['format'] = list(pd.Series(summary_count.index.values).apply(lambda x: dataset2format[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_correct</th>\n",
       "      <th>all_wrong</th>\n",
       "      <th>all_either</th>\n",
       "      <th>format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adversarial_nli</th>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.011875</td>\n",
       "      <td>0.012188</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arc_easy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>MC-par</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boolq</th>\n",
       "      <td>0.095413</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>MC-par</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosmosqa</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>MC-par</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hellaswag</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>MC-par</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mctaco</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>MC-sent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnli</th>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrqa_natural_questions</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073076</td>\n",
       "      <td>0.073076</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsqa</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161426</td>\n",
       "      <td>0.161426</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qamr</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080714</td>\n",
       "      <td>0.080714</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quoref</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026468</td>\n",
       "      <td>0.026468</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snli</th>\n",
       "      <td>0.031759</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.033388</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>squad_v2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.005715</td>\n",
       "      <td>0.036992</td>\n",
       "      <td>0.042707</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        all_correct  all_wrong  all_either          format\n",
       "adversarial_nli            0.000313   0.011875    0.012188  classification\n",
       "arc_easy                   0.000000   0.000421    0.000421          MC-par\n",
       "boolq                      0.095413   0.002446    0.097859          MC-par\n",
       "cosmosqa                   0.000000   0.002009    0.002009          MC-par\n",
       "hellaswag                  0.000000   0.000398    0.000398          MC-par\n",
       "mctaco                     0.000000   0.108108    0.108108         MC-sent\n",
       "mnli                       0.000102   0.000407    0.000509  classification\n",
       "mrqa_natural_questions     0.000000   0.073076    0.073076  span selection\n",
       "newsqa                     0.000000   0.161426    0.161426  span selection\n",
       "qamr                       0.000000   0.080714    0.080714  span selection\n",
       "quoref                     0.000000   0.026468    0.026468  span selection\n",
       "snli                       0.031759   0.001629    0.033388  classification\n",
       "squad_v2                   0.000000   0.021322    0.021322  span selection\n",
       "all                        0.005715   0.036992    0.042707             all"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_pcent.loc[summary_pcent['all_either'] > 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_correct</th>\n",
       "      <th>all_wrong</th>\n",
       "      <th>all_either</th>\n",
       "      <th>total_count</th>\n",
       "      <th>format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adversarial_nli</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>3200</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arc_easy</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2376</td>\n",
       "      <td>MC-par</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boolq</th>\n",
       "      <td>156</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>1635</td>\n",
       "      <td>MC-par</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosmosqa</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1493</td>\n",
       "      <td>MC-par</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hellaswag</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5021</td>\n",
       "      <td>MC-par</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mctaco</th>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>1332</td>\n",
       "      <td>MC-sent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnli</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9824</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrqa_natural_questions</th>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>469</td>\n",
       "      <td>6418</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newsqa</th>\n",
       "      <td>0</td>\n",
       "      <td>693</td>\n",
       "      <td>693</td>\n",
       "      <td>4293</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qamr</th>\n",
       "      <td>0</td>\n",
       "      <td>1515</td>\n",
       "      <td>1515</td>\n",
       "      <td>18770</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quoref</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>1209</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snli</th>\n",
       "      <td>312</td>\n",
       "      <td>16</td>\n",
       "      <td>328</td>\n",
       "      <td>9824</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>squad_v2</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>5675</td>\n",
       "      <td>span selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>470</td>\n",
       "      <td>3042</td>\n",
       "      <td>3512</td>\n",
       "      <td>82234</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        all_correct  all_wrong  all_either  total_count  \\\n",
       "adversarial_nli                   1         38          39         3200   \n",
       "arc_easy                          0          1           1         2376   \n",
       "boolq                           156          4         160         1635   \n",
       "cosmosqa                          0          3           3         1493   \n",
       "hellaswag                         0          2           2         5021   \n",
       "mctaco                            0        144         144         1332   \n",
       "mnli                              1          4           5         9824   \n",
       "mrqa_natural_questions            0        469         469         6418   \n",
       "newsqa                            0        693         693         4293   \n",
       "qamr                              0       1515        1515        18770   \n",
       "quoref                            0         32          32         1209   \n",
       "snli                            312         16         328         9824   \n",
       "squad_v2                          0        121         121         5675   \n",
       "all                             470       3042        3512        82234   \n",
       "\n",
       "                                format  \n",
       "adversarial_nli         classification  \n",
       "arc_easy                        MC-par  \n",
       "boolq                           MC-par  \n",
       "cosmosqa                        MC-par  \n",
       "hellaswag                       MC-par  \n",
       "mctaco                         MC-sent  \n",
       "mnli                    classification  \n",
       "mrqa_natural_questions  span selection  \n",
       "newsqa                  span selection  \n",
       "qamr                    span selection  \n",
       "quoref                  span selection  \n",
       "snli                    classification  \n",
       "squad_v2                span selection  \n",
       "all                                all  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_count.loc[summary_count['all_either'] > 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_by_format = summary_count.groupby(by='format')\n",
    "summary_by_format = summary_by_format.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_correct</th>\n",
       "      <th>all_wrong</th>\n",
       "      <th>all_either</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MC-par</th>\n",
       "      <td>156</td>\n",
       "      <td>10</td>\n",
       "      <td>166</td>\n",
       "      <td>17193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC-sent</th>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>5660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>470</td>\n",
       "      <td>3042</td>\n",
       "      <td>3512</td>\n",
       "      <td>82234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classification</th>\n",
       "      <td>314</td>\n",
       "      <td>58</td>\n",
       "      <td>372</td>\n",
       "      <td>23015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>span selection</th>\n",
       "      <td>0</td>\n",
       "      <td>2830</td>\n",
       "      <td>2830</td>\n",
       "      <td>36365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                all_correct  all_wrong  all_either  total_count\n",
       "format                                                         \n",
       "MC-par                  156         10         166        17193\n",
       "MC-sent                   0        144         144         5660\n",
       "all                     470       3042        3512        82234\n",
       "classification          314         58         372        23015\n",
       "span selection            0       2830        2830        36365"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_by_format.loc[summary_by_format['all_either'] > 0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break Down of All Either\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "format\n",
       "MC-par            0.047267\n",
       "MC-sent           0.041002\n",
       "all               1.000000\n",
       "classification    0.105923\n",
       "span selection    0.805809\n",
       "Name: all_either, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Break Down of All Either')\n",
    "summary_by_format['all_either']/summary_by_format.loc['all', 'all_either']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakdown of All Either by Task Format\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "format\n",
       "MC-par            0.009655\n",
       "MC-sent           0.025442\n",
       "all               0.042707\n",
       "classification    0.016163\n",
       "span selection    0.077822\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Breakdown of All Either by Task Format')\n",
    "summary_by_format['all_either']/summary_by_format['total_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('..','data')\n",
    "files = [f for f in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Trim Model Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trimmed_model_dir = os.path.join('..','data_trimmed_model')\n",
    "os.makedirs(trimmed_model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exclude_models = ['albert-xxlarge-v2', 'xlm-roberta-large', 'roberta-large', 'roberta-base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    temp = pd.read_csv(os.path.join(data_dir, file))\n",
    "    temp_models = temp['userid'].apply(lambda x:''.join(x.split('_')[:-1]))\n",
    "    \n",
    "    exclude_indexes = temp_models.eq(exclude_models[0])\n",
    "    for i in range(1,len(exclude_models)):\n",
    "        exclude_indexes = exclude_indexes + temp_models.eq(exclude_models[i])\n",
    "    \n",
    "    trimmed = temp.loc[~exclude_indexes, :]\n",
    "    \n",
    "    check = set(temp.loc[~exclude_indexes, :]['userid'].apply(lambda x:''.join(x.split('_')[:-1])).unique())\n",
    "    for model in exclude_models:\n",
    "        assert not model in check, f'{file}, {model}'\n",
    "    \n",
    "    trimmed.to_csv(os.path.join(trimmed_model_dir, file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Trim Item Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trimmed_item_dir = os.path.join('..','data_trimmed_item')\n",
    "os.makedirs(trimmed_item_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ending = '_irt_all_coded.csv'\n",
    "\n",
    "for file in files:\n",
    "    dataset_name = '_'.join(file[:-len(ending)].split('-'))\n",
    "    if dataset_name == 'mrqa_nq':\n",
    "        dataset_name = 'mrqa_natural_questions'\n",
    "    \n",
    "    if not dataset_name in item_excludes.keys():\n",
    "        print(dataset_name)  \n",
    "    \n",
    "    temp = pd.read_csv(os.path.join(data_dir, file))\n",
    "    trimmed = temp.loc[:,~np.array([False] + list(item_excludes[dataset_name]))]\n",
    "    \n",
    "    trimmed.to_csv(os.path.join(trimmed_item_dir, file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEH at BERT-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Grad for Target Model\n",
      "Target model: bert-large-cased_best\n",
      "0.7179280519485474\n",
      "No gamma: 0.1566200056320648\n",
      "With gamma: 0.12033600391590196\n",
      "           mean\n",
      "0      0.173203\n",
      "1      0.169138\n",
      "2      0.056116\n",
      "3      0.046104\n",
      "4      0.129236\n",
      "...         ...\n",
      "82228  0.087480\n",
      "82229  0.152120\n",
      "82230  0.120044\n",
      "82231  0.037668\n",
      "82232  0.050374\n",
      "\n",
      "[82233 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "target = 'bert-large-cased_best'\n",
    "\n",
    "leh_scores_bert = icc_best_deriv(\n",
    "    param_plot_stats['a'],\n",
    "    param_plot_stats['b'],\n",
    "    param_plot_stats['t'],\n",
    "    list(combined_responses['userid']),\n",
    "    gamma = param_plot_stats['g'],\n",
    "    target = target\n",
    ")\n",
    "\n",
    "leh_scores_plot_bert = pd.DataFrame(pd.Series(leh_scores_bert), columns = ['mean'])\n",
    "print(leh_scores_plot_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Trimmed BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to False if run for the first time\n",
    "# note that this will take sometimes to run if the datasets are big\n",
    "load_from_cache_trimmed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution and transformation\n",
    "alpha_dist = 'log-normal'\n",
    "alpha_transf = 'standard'\n",
    "theta_dist = 'normal'\n",
    "theta_transf = 'standard'\n",
    "\n",
    "exp_dir_trimmed = os.path.join(repo, 'params_trimmed_model', f'alpha-lognormal-identity_theta-normal-identity_nosubsample_1.00_0.30')\n",
    "p = 0.95\n",
    "\n",
    "with open(os.path.join(exp_dir_trimmed, 'responses.p'), 'rb') as f:\n",
    "    combined_responses_trimmed = pickle.load(f).reset_index()\n",
    "data_trimmed, accuracies_trimmed, example_accuracies_trimmed = get_data_accuracies(combined_responses_trimmed)\n",
    "column_names_trimmed = combined_responses_trimmed.columns[1:]\n",
    "select_ts = {\n",
    "    'standard':lambda x:x,\n",
    "    'positive':lambda x:torch.log(1+torch.exp(torch.tensor(x))),\n",
    "    'sigmoid':lambda x:sigmoid(torch.tensor(x)),\n",
    "}\n",
    "\n",
    "transforms = {\n",
    "    'a':select_ts[alpha_transf],\n",
    "    'b':select_ts['standard'],\n",
    "    'g':select_ts['sigmoid'],\n",
    "    't':select_ts[theta_transf],\n",
    "}\n",
    "\n",
    "if load_from_cache_trimmed:\n",
    "    param_plot_stats_trimmed = {}\n",
    "\n",
    "    for key in transforms.keys():\n",
    "        with open(os.path.join('plot_stats_pickles_trimmed_model', f'{key}.p'), 'rb') as f:\n",
    "            param_plot_stats_trimmed[key] = pickle.load(f)\n",
    "else:\n",
    "    param_plot_stats_trimmed = get_plot_stats(\n",
    "        exp_dir_trimmed,\n",
    "        alpha_dist,\n",
    "        theta_dist,\n",
    "        transforms,\n",
    "        p = p\n",
    "    )\n",
    "    \n",
    "    os.makedirs('plot_stats_pickles_trimmed_model', exist_ok=True)\n",
    "    for key, value in param_plot_stats.items():\n",
    "        with open(os.path.join('plot_stats_pickles_trimmed_model', f'{key}.p'), 'wb') as f:\n",
    "            pickle.dump(value, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Grad for Target Model\n",
      "Target model: bert-large-cased_best\n",
      "0.9205797910690308\n",
      "No gamma: 0.1568617125845869\n",
      "With gamma: 0.12112190022703949\n",
      "           mean\n",
      "0      0.155609\n",
      "1      0.131961\n",
      "2      0.056406\n",
      "3      0.058805\n",
      "4      0.128713\n",
      "...         ...\n",
      "82228  0.097181\n",
      "82229  0.166697\n",
      "82230  0.142921\n",
      "82231  0.122178\n",
      "82232  0.069321\n",
      "\n",
      "[82233 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "target = 'bert-large-cased_best'\n",
    "\n",
    "leh_scores_bert_trimmed = icc_best_deriv(\n",
    "    param_plot_stats_trimmed['a'],\n",
    "    param_plot_stats_trimmed['b'],\n",
    "    param_plot_stats_trimmed['t'],\n",
    "    list(combined_responses_trimmed['userid']),\n",
    "    gamma = param_plot_stats_trimmed['g'],\n",
    "    target = target\n",
    ")\n",
    "\n",
    "leh_scores_plot_bert_trimmed = pd.DataFrame(pd.Series(leh_scores_bert_trimmed), columns = ['mean'])\n",
    "print(leh_scores_plot_bert_trimmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "leh_bert_combined = pd.concat([leh_scores_plot_bert, leh_scores_plot_bert_trimmed], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean -0.0007858963111375089\n",
      "std 0.04295326081138878\n",
      "median -0.0017299276058100402\n",
      "IQR 0.04448709300453024\n",
      "upper_whisker 0.08537090195616145\n",
      "lower_whisker -0.09257747006195952\n",
      "pcent above lower 0.040166356572179054\n",
      "pcent below lower 0.011394452348813736\n"
     ]
    }
   ],
   "source": [
    "diff = (leh_scores_plot_bert['mean'] - leh_scores_plot_bert_trimmed['mean'])\n",
    "\n",
    "print('mean', diff.mean())\n",
    "print('std', diff.std())\n",
    "print('median', diff.median())\n",
    "\n",
    "IQR = diff.quantile(0.75) - diff.quantile(0.25)\n",
    "upper_whisker = diff.quantile(0.75) + 1.5*IQR\n",
    "lower_whisker = diff.quantile(0.25) - 1.5*IQR\n",
    "\n",
    "print('IQR', IQR)\n",
    "print('upper_whisker', upper_whisker)\n",
    "print('lower_whisker', lower_whisker)\n",
    "\n",
    "print('pcent above lower', (diff > upper_whisker).sum()/diff.shape[0])\n",
    "print('pcent below lower', (diff < lower_whisker).sum()/diff.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f737c7d772431ba5b528cfcfd5c45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'LEH at BERT: Difference')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plot.subplots()\n",
    "sns.boxplot(diff)\n",
    "ax.set_xlabel(r'$\\Delta$ LEH')\n",
    "ax.set_title('LEH at BERT: Difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a89283b1a0942a78e70a1bc43852c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'LEH at BERT: Difference')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plot.subplots()\n",
    "sns.boxplot(diff, showfliers=False)\n",
    "ax.set_xlabel(r'$\\Delta$ LEH')\n",
    "ax.set_title('LEH at BERT: Difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0.04818176671356783\n",
      "std 0.48766134206164674\n",
      "median -0.020584120997042255\n",
      "IQR 0.45042455248578395\n",
      "upper_whisker 0.8926601474803466\n",
      "lower_whisker -0.9090380624627892\n",
      "pcent above lower 0.05154864835285104\n",
      "pcent below lower 3.6481704425230744e-05\n"
     ]
    }
   ],
   "source": [
    "rel_diff = (leh_scores_plot_bert['mean'] - leh_scores_plot_bert_trimmed['mean'])/leh_scores_plot_bert_trimmed['mean']\n",
    "\n",
    "print('mean', rel_diff.mean())\n",
    "print('std', rel_diff.std())\n",
    "print('median', rel_diff.median())\n",
    "\n",
    "IQR = rel_diff.quantile(0.75) - rel_diff.quantile(0.25)\n",
    "upper_whisker = rel_diff.quantile(0.75) + 1.5*IQR\n",
    "lower_whisker = rel_diff.quantile(0.25) - 1.5*IQR\n",
    "\n",
    "print('IQR', IQR)\n",
    "print('upper_whisker', upper_whisker)\n",
    "print('lower_whisker', lower_whisker)\n",
    "\n",
    "print('pcent above lower', (rel_diff > upper_whisker).sum()/rel_diff.shape[0])\n",
    "print('pcent below lower', (rel_diff < lower_whisker).sum()/rel_diff.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96d725cd9cf451ca2a8d26eb9db83e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'LEH at BERT: Relative Difference')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plot.subplots()\n",
    "sns.boxplot(rel_diff)\n",
    "ax.set_xlabel(r'$\\Delta$ LEH / LEH$_{BERT}$')\n",
    "ax.set_title('LEH at BERT: Relative Difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b637ea0ac294eec86a34b7fcbbd0e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'LEH at BERT: Relative Difference')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plot.subplots()\n",
    "sns.boxplot(rel_diff, showfliers=False)\n",
    "ax.set_xlabel(r'$\\Delta$ LEH / LEH$_{BERT}$')\n",
    "ax.set_title('LEH at BERT: Relative Difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_label = 12\n",
    "font_legend = 10\n",
    "font_legendtitle = font_legend + 4\n",
    "font_xtick = 12\n",
    "font_title = 14\n",
    "marker_scale = 1.5\n",
    "\n",
    "plot.rc('axes', labelsize=font_label)\n",
    "plot.rc('axes', titlesize=font_title)\n",
    "plot.rc('legend', fontsize=font_legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(df, order, param_type, task_metadata, xsize=12, ysize=3, width=0.6, rotation=90, ylim=None, ystep=None):\n",
    "    \n",
    "    param2label = {\n",
    "        'discriminative':r'$\\log$ Discrimination ($\\log$ $\\alpha$)',\n",
    "        'difficulty': r'Difficulty ($\\beta$)',\n",
    "        \"disc-diff\": \"IRT Score\", # \"discrimination - difficulty product\\n\", # + r\"($ n(\\alpha) x n(\\beta$) )\"\n",
    "        \"disc-diff_pos\": \"IRT Score\", # \"discrimination - difficulty product\\n\", # + r\"($ n(\\alpha) x n(\\beta$) )\"\n",
    "        \"disc-diff_minmax\": \"IRT Score\", # \"discrimination - difficulty product\\n\", # + r\"($ n(\\alpha) x n(\\beta$) )\"\n",
    "        \"irt-score\": \"LEH Score\",\n",
    "    }\n",
    "    \n",
    "    param2yname = {\n",
    "        'discriminative': \"log_mean\",\n",
    "        'difficulty': \"mean\",\n",
    "        \"disc-diff\": 0,\n",
    "        \"disc-diff_pos\": 0,\n",
    "        \"disc-diff_minmax\": 0,\n",
    "        \"irt-score\": \"mean\",\n",
    "    }\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "    f, ax = plot.subplots(figsize=(xsize, ysize))\n",
    "    \n",
    "    my_pal = {\"MC-par\": \"r\",\n",
    "              \"MC-sent\": \"b\",\n",
    "              \"classification\":\"g\",\n",
    "              \"span selection\": \"grey\"}    \n",
    "    \n",
    "    ax = sns.boxplot(x=\"task_name\", y=param2yname[param_type], data=df, order=order, width=width)\n",
    "    \n",
    "    for i, task in enumerate(order):\n",
    "        # Select which box you want to change    \n",
    "        mybox = ax.artists[i]\n",
    "        \n",
    "        # Change the appearance of that box\n",
    "        skill = task_metadata.loc[task]['format']\n",
    "        mybox.set_facecolor(my_pal[skill])\n",
    "    \n",
    "    # Add transparency to colors\n",
    "    for patch in ax.artists:\n",
    "         r, g, b, a = patch.get_facecolor()\n",
    "         patch.set_facecolor((r, g, b, .6))\n",
    "\n",
    "    sns.despine()\n",
    "    plot.xticks(range(len(order)), order, rotation=rotation, fontsize=font_xtick)\n",
    "    \n",
    "    if not ylim is None and not ystep is None:\n",
    "        plot.ylim(ylim)\n",
    "        plot.yticks(numpy.arange(ylim[0], ylim[1]+ystep, ystep))\n",
    "    \n",
    "    plot.xlabel(None)\n",
    "    plot.ylabel(param2label[param_type], fontsize=font_label)\n",
    "    \n",
    "    return f\n",
    "#     plot.savefig('../plots/' + param_type + \"_box.png\",\n",
    "#                 format='png', dpi=300,\n",
    "#                 bbox_inches = 'tight',\n",
    "#                 pad_inches = .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.851996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.851996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean      mean\n",
       "mean  1.000000  0.851996\n",
       "mean  0.851996  1.000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leh_bert_combined.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leh_scores_plot_bert = pd.concat([leh_scores_plot_bert, task_name_format], axis=1)\n",
    "leh_scores_plot_bert_trimmed = pd.concat([leh_scores_plot_bert_trimmed, task_name_format], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506c39f3f089475984530f41de870c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plot_box(\n",
    "    leh_scores_plot_bert,\n",
    "    task_order,\n",
    "    \"irt-score\",\n",
    "    task_metadata\n",
    ")\n",
    "\n",
    "f.suptitle('All Models')\n",
    "f.savefig(os.path.join('..', 'plots_LEH_BERT', 'all_models.png'),\n",
    "         format='png', dpi=300,\n",
    "                bbox_inches = 'tight',\n",
    "                pad_inches = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df52f678d004dca9dac74036ef11637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ftrimmed = plot_box(\n",
    "    leh_scores_plot_bert_trimmed,\n",
    "    task_order,\n",
    "    \"irt-score\",\n",
    "    task_metadata\n",
    ")\n",
    "\n",
    "ftrimmed.suptitle('MiniBERTas + BERT')\n",
    "ftrimmed.savefig(os.path.join('..', 'plots_LEH_BERT', 'trimmed_models.png'),\n",
    "         format='png', dpi=300,\n",
    "                bbox_inches = 'tight',\n",
    "                pad_inches = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "leh_tasks = pd.concat([\n",
    "    leh_scores_plot_bert.rename(columns={'mean':'Full'}),\n",
    "    pd.DataFrame(leh_scores_plot_bert_trimmed['mean']).rename(columns={'mean':'BERT'})\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff 0.0004319164170291101\n",
      "rel diff 0.003139606438779691\n"
     ]
    }
   ],
   "source": [
    "leh_tasks_qtile = leh_tasks.groupby(by='task_name').quantile(q=0.75)\n",
    "leh_tasks_qtile['diff'] = leh_tasks_qtile['Full'] - leh_tasks_qtile['BERT']\n",
    "leh_tasks_qtile['rel_diff'] = (leh_tasks_qtile['Full'] - leh_tasks_qtile['BERT'])/leh_tasks_qtile['BERT']\n",
    "\n",
    "print('diff', leh_tasks_qtile['diff'].median())\n",
    "print('rel diff', leh_tasks_qtile['rel_diff'].median())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
