{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle5 as pickle\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy\n",
    "import torch\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "import pyro\n",
    "import pyro.infer\n",
    "import pyro.infer.mcmc\n",
    "import pyro.distributions as dist\n",
    "import torch.distributions.constraints as constraints\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "repo = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1.+torch.exp(-x))\n",
    "\n",
    "def icc_best_deriv(alpha, beta, theta, model_names, gamma=None, col='mean'):\n",
    "    '''\n",
    "    Method to calculate the locally estimated headroom (LEH) score, defined as\n",
    "    the derivative of the item characteristic curve w.r.t. the best performing model.\n",
    "    \n",
    "    Args:\n",
    "        alpha:       DataFrame of discrimination parameter statistics for each item.\n",
    "        beta:        DataFrame of difficulty parameter statistics for each item.\n",
    "        theta:       DataFrame of ability parameter statistics for each responder.\n",
    "        model_names: List of responder names.\n",
    "        gamma:       DataFrame of guessing parameter statistics for each item.\n",
    "        col:         DataFrame column name to use for calculating LEH scores.\n",
    "    \n",
    "    Returns:\n",
    "        scores:      LEH scores for each item.    \n",
    "    '''\n",
    "    best_idx, best_value = theta[col].argmax(), theta[col].max()\n",
    "    print(f'Best model: {model_names[best_idx]}\\n{best_value}')\n",
    "    \n",
    "    a, b = torch.tensor(alpha[col].values), torch.tensor(beta[col].values)\n",
    "    \n",
    "    logits = (a*(best_value-b))\n",
    "    sigmoids = sigmoid(logits)\n",
    "    scores = sigmoids*(1.-sigmoids)*a\n",
    "    \n",
    "    print(f'No gamma: {scores.mean()}')\n",
    "    if not gamma is None:\n",
    "        g = torch.tensor(gamma[col].apply(lambda x: x.item()).values)\n",
    "        scores = (1.-g)*scores\n",
    "        print(f'With gamma: {scores.mean()}')\n",
    "    \n",
    "    return scores      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_model_guide(alpha_dist, theta_dist, alpha_transform, theta_transform):\n",
    "    model = lambda obs: irt_model(obs, alpha_dist, theta_dist, alpha_transform = alpha_transform, theta_transform = theta_transform)\n",
    "    guide = lambda obs: vi_posterior(obs, alpha_dist, theta_dist)\n",
    "    \n",
    "    return model, guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_data_accuracies(data, verbose = False, get_cols = False):\n",
    "    '''\n",
    "    Method to reformat `data` and calculate item and responder accuracies.\n",
    "    \n",
    "    Args:\n",
    "        data:                DataFrame of item responses.\n",
    "        verbose:             Boolean value of whether to print statements.\n",
    "        get_cols:            Boolean value of whether to return original column\n",
    "                             values of `data`.\n",
    "        \n",
    "    Returns:\n",
    "        new_data:            Reformatted `data`, dropping first column.\n",
    "        accuracies:          Accuracy for each responder across examples.\n",
    "        example_accuracies:  Accuracy for each example across responders.\n",
    "        data.columns.values: Returns only if `get_cols` is True. Original column\n",
    "                             values of `data`.\n",
    "    '''\n",
    "    new_data = numpy.array(data)\n",
    "    new_data = new_data[:,1:]\n",
    "    \n",
    "    model_names = dict(data['userid'])\n",
    "    accuracies = new_data.mean(-1)\n",
    "    example_accuracies = new_data.mean(0)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n'.join([f'{name}: {acc}' for name, acc in zip(model_names.values(),accuracies)]))\n",
    "    \n",
    "    if get_cols:\n",
    "        return new_data, accuracies, example_accuracies, data.columns.values\n",
    "    else:\n",
    "        return new_data, accuracies, example_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_stats_CI(params, p=0.95, dist='normal'):\n",
    "    '''\n",
    "    Method to calculate lower and upper quantiles defined by `p`, mean, and variance of `param`\n",
    "    \n",
    "    Args:\n",
    "        params: Dictionary of distribution parameters for each item keyed according to the \n",
    "                parametric distribution defined by `dist`.\n",
    "        p:      Percent of distribution covered by the lower and upper interval values for each\n",
    "                parameter.\n",
    "        dist:   Name of parametric distribution\n",
    "    \n",
    "    Returns:\n",
    "        return: {\n",
    "            'lower': Lower interval values of each parameter,\n",
    "            'upper': Upper interval values of each parameter,\n",
    "            'mean' : Mean of each parameter,\n",
    "            'var'  : Variance of each parameter\n",
    "        }\n",
    "    '''\n",
    "    stats = {}\n",
    "    if dist == 'normal':\n",
    "        L,U = scipy.stats.norm.interval(p,loc=params['mu'], scale=torch.exp(params['logstd']))\n",
    "        M,V = scipy.stats.norm.stats(loc=params['mu'], scale=torch.exp(params['logstd']))\n",
    "    elif dist == 'log-normal':\n",
    "        L,U = scipy.stats.lognorm.interval(p, s=torch.exp(params['logstd']), scale=torch.exp(params['mu']))\n",
    "        M,V = scipy.stats.lognorm.stats(s=torch.exp(params['logstd']), scale=torch.exp(params['mu']))\n",
    "    elif dist == 'beta':\n",
    "        L,U = scipy.stats.beta.interval(p,a=params['alpha'], b=params['beta'])\n",
    "        M,V = scipy.stats.beta.stats(a=params['alpha'], b=params['beta'])\n",
    "    else:\n",
    "        raise TypeError(f'Distribution type {dist} not supported.')\n",
    "    \n",
    "    return {\n",
    "        'lower':[L],\n",
    "        'upper':[U],\n",
    "        'mean':[M],\n",
    "        'var':[V],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_plot_stats(exp_dir, alpha_dist, theta_dist, transforms, p = 0.95):\n",
    "    '''\n",
    "    Method to return plotting statistics for 3 parameter IRT model parameters.\n",
    "    \n",
    "    Args:\n",
    "        exp_dir:          Path to 3 parameter IRT parameters and responses.\n",
    "        alpha_dist:       Name of the item discrimination [a] distribution.\n",
    "        theta_dist:       Name of the responder ability [t] distribution.\n",
    "        transforms:       Dictionary of transformations to apply to each parameter type\n",
    "                          where keys are parameter names and values are functions.\n",
    "        p:                Percent of distribution covered by the lower and upper interval \n",
    "                          values for each parameter.\n",
    "    \n",
    "    Returns:\n",
    "        param_plot_stats: Dictionary of parameter plot statistics where keys are parameter\n",
    "                          names and values are plot statistics dictionaries as defined by\n",
    "                          get_stats_CI().\n",
    "    '''\n",
    "    param_dists = {\n",
    "        'a':alpha_dist,\n",
    "        'b':'normal',\n",
    "        'g':'normal',\n",
    "        't':theta_dist,\n",
    "    }\n",
    "\n",
    "    dist_params = {\n",
    "        'normal':['mu', 'logstd'],\n",
    "        'log-normal':['mu', 'logstd'],\n",
    "        'beta':['alpha', 'beta'],\n",
    "    }\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "    pyro.get_param_store().load(os.path.join(exp_dir, 'params.p'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pyro_param_dict = dict(pyro.get_param_store().named_parameters())\n",
    "    \n",
    "    # get stats for plotting\n",
    "    param_plot_stats = {}\n",
    "\n",
    "    for param, param_dist in param_dists.items():\n",
    "        temp_params = dist_params[param_dist]\n",
    "\n",
    "        for idx, (p1_orig, p2_orig) in enumerate(zip(pyro_param_dict[f'{param} {temp_params[0]}'], pyro_param_dict[f'{param} {temp_params[1]}'])):\n",
    "            p1, p2 = p1_orig.detach(), p2_orig.detach()\n",
    "            \n",
    "            temp_stats_df = pd.DataFrame.from_dict(\n",
    "                get_stats_CI(\n",
    "                    params = {\n",
    "                        temp_params[0]:p1,\n",
    "                        temp_params[1]:p2,\n",
    "                    },\n",
    "                    p=p,\n",
    "                    dist = param_dist,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            temp_stats_df = temp_stats_df.applymap(transforms[param])\n",
    "        \n",
    "            if idx == 0:\n",
    "                param_plot_stats[param] = temp_stats_df\n",
    "            else:\n",
    "                param_plot_stats[param] = param_plot_stats[param].append(temp_stats_df, ignore_index = True)\n",
    "    \n",
    "    return param_plot_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sign_mult(df1, df2):\n",
    "    newdf = copy.deepcopy(df2)\n",
    "    \n",
    "    for idx, row in df1.iterrows():\n",
    "        if numpy.sign(row['mean']) < 0:\n",
    "            newdf.loc[idx,'mean'] = -1*newdf.loc[idx,'mean']\n",
    "            newdf.loc[idx,'lower'] = -1*newdf.loc[idx,'upper']\n",
    "            newdf.loc[idx,'upper'] = -1*newdf.loc[idx,'lower']\n",
    "    \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_diff_by_set(diffs, item_ids):\n",
    "    diff_by_set = {}\n",
    "    id_split = '_'\n",
    "\n",
    "    max_diff = -1e6\n",
    "    min_diff = 1e6\n",
    "    \n",
    "    for idx, diff in enumerate(diffs):\n",
    "        set_name = item_ids[idx].split(id_split)[0]\n",
    "\n",
    "        if set_name in diff_by_set.keys():\n",
    "            diff_by_set[set_name].append(diff)\n",
    "        else:\n",
    "            diff_by_set[set_name] = [diff]\n",
    "            \n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            \n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "    \n",
    "    return diff_by_set, min_diff, max_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variational_irt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=\"boolq,cb,commonsenseqa,copa,cosmosqa,hellaswag,adversarial-nli,rte,snli,wic,qamr,arct,mcscript,mctaco,mutual,mutual-plus,quoref,socialiqa,squad-v2,wsc,mnli,mrqa-nq,newsqa,abductive-nli,arc-easy,arc-challenge,piqa,quail,winogrande,anli\"\n",
    "data_names, responses, n_items = get_files(\n",
    "    os.path.join(repo, 'data_trimmed_item'),\n",
    "    \"csv\",\n",
    "    set(datasets.split(','))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metadata = pd.read_csv('task_metadata.csv')\n",
    "task_metadata.set_index(\"jiant_name\", inplace=True)\n",
    "task_list = [x for x in task_metadata.index if x in data_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "task_name = []\n",
    "task_format = []\n",
    "\n",
    "for tname, size in zip(data_names, n_items):\n",
    "    name = task_metadata.loc[tname]['taskname']\n",
    "    total += size\n",
    "    task_name += [name for _ in range(size)]\n",
    "    task_format += [task_metadata.loc[tname]['format'] for _ in range(size)]\n",
    "    \n",
    "task_name = pd.DataFrame(task_name, columns=['task_name'])\n",
    "task_format = pd.DataFrame(task_format, columns=['format'])\n",
    "task_name_format_trimmed = pd.concat([task_name, task_format], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Params and Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = os.path.join(repo, 'params_trimmed_item', f'alpha-lognormal-identity_theta-normal-identity_nosubsample_1.00_0.30')\n",
    "p = 0.95\n",
    "\n",
    "with open(os.path.join(exp_dir, 'responses.p'), 'rb') as f:\n",
    "    combined_responses = pickle.load(f).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-large_best\n",
      "==========================================================================================\n",
      "Overall acc: 0.7661\n",
      "abductive_nli acc: 0.8564\n",
      "adversarial_nli acc: 0.4995\n",
      "arc_challenge acc: 0.3319\n",
      "arc_easy acc: 0.6299\n",
      "arct acc: 0.8604\n",
      "boolq acc: 0.8217\n",
      "cb acc: 0.8571\n",
      "commonsenseqa acc: 0.6759\n",
      "copa acc: 0.8400\n",
      "cosmosqa acc: 0.8000\n",
      "hellaswag acc: 0.8420\n",
      "mcscript acc: 0.9183\n",
      "mctaco acc: 0.6010\n",
      "mnli acc: 0.8995\n",
      "mrqa_natural_questions acc: 0.7489\n",
      "mutual_plus acc: 0.7314\n",
      "mutual acc: 0.8668\n",
      "newsqa acc: 0.6608\n",
      "piqa acc: 0.7617\n",
      "qamr acc: 0.7944\n",
      "quail acc: 0.6691\n",
      "quoref acc: 0.8241\n",
      "rte acc: 0.8345\n",
      "snli acc: 0.9192\n",
      "socialiqa acc: 0.7738\n",
      "squad_v2 acc: 0.4337\n",
      "wic acc: 0.7085\n",
      "winogrande acc: 0.7697\n",
      "wsc acc: 0.6154\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy of roberta-large models\n",
    "\n",
    "extractmodel = 'roberta-large_best'\n",
    "tie_break = 0\n",
    "\n",
    "acc_by_dataset = {}\n",
    "\n",
    "roberta_rp = combined_responses.loc[combined_responses['userid']==extractmodel, :]\n",
    "if roberta_rp.shape[0] > 1:\n",
    "    roberta_rp = roberta_rp.iloc[tie_break, :]\n",
    "\n",
    "cols = combined_responses.columns.values\n",
    "\n",
    "for item in cols[1:]:\n",
    "    data_name = '_'.join(item.split('_')[:-1])\n",
    "    resp = roberta_rp[item].item()\n",
    "    \n",
    "    if data_name in acc_by_dataset:\n",
    "        acc_by_dataset[data_name]['correct'] += resp\n",
    "        acc_by_dataset[data_name]['total'] += 1\n",
    "    else:\n",
    "        acc_by_dataset[data_name] = {'correct': resp, 'total': 1}\n",
    "\n",
    "print(extractmodel)\n",
    "print('='*90)\n",
    "print(f'Overall acc: {roberta_rp.iloc[0, 1:].sum()/(roberta_rp.shape[1]-1):.4f}')        \n",
    "\n",
    "for data_name, acc_dict in acc_by_dataset.items():\n",
    "    print(f'{data_name} acc: {acc_dict[\"correct\"]/acc_dict[\"total\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to False if run for the first time\n",
    "# note that this will take sometimes to run if the datasets are big\n",
    "load_from_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution and transformation\n",
    "alpha_dist = 'log-normal'\n",
    "alpha_transf = 'standard'\n",
    "theta_dist = 'normal'\n",
    "theta_transf = 'standard'\n",
    "\n",
    "exp_dir = os.path.join(repo, 'params_trimmed_item', f'alpha-lognormal-identity_theta-normal-identity_nosubsample_1.00_0.30')\n",
    "p = 0.95\n",
    "\n",
    "with open(os.path.join(exp_dir, 'responses.p'), 'rb') as f:\n",
    "    combined_responses = pickle.load(f).reset_index()\n",
    "data, accuracies, example_accuracies = get_data_accuracies(combined_responses)\n",
    "column_names = combined_responses.columns[1:]\n",
    "select_ts = {\n",
    "    'standard':lambda x:x,\n",
    "    'positive':lambda x:torch.log(1+torch.exp(torch.tensor(x))),\n",
    "    'sigmoid':lambda x:sigmoid(torch.tensor(x)),\n",
    "}\n",
    "\n",
    "transforms = {\n",
    "    'a':select_ts[alpha_transf],\n",
    "    'b':select_ts['standard'],\n",
    "    'g':select_ts['sigmoid'],\n",
    "    't':select_ts[theta_transf],\n",
    "}\n",
    "\n",
    "if load_from_cache:\n",
    "    param_plot_stats_trimmed = {}\n",
    "\n",
    "    for key in transforms.keys():\n",
    "        with open(os.path.join('plot_stats_pickles_trimmed_item', f'{key}.p'), 'rb') as f:\n",
    "            param_plot_stats_trimmed[key] = pickle.load(f)\n",
    "else:\n",
    "    param_plot_stats = get_plot_stats(\n",
    "        exp_dir,\n",
    "        alpha_dist,\n",
    "        theta_dist,\n",
    "        transforms,\n",
    "        p = 0.95\n",
    "    )\n",
    "    \n",
    "    os.makedirs('plot_stats_pickles_trimmed_item', exist_ok=True)\n",
    "    for key, value in param_plot_stats.items():\n",
    "        with open(os.path.join('plot_stats_pickles_trimmed_item', f'{key}.p'), 'wb') as f:\n",
    "            pickle.dump(value, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>abductive_nli_0</th>\n",
       "      <th>abductive_nli_1</th>\n",
       "      <th>abductive_nli_2</th>\n",
       "      <th>abductive_nli_3</th>\n",
       "      <th>abductive_nli_4</th>\n",
       "      <th>abductive_nli_5</th>\n",
       "      <th>abductive_nli_6</th>\n",
       "      <th>abductive_nli_7</th>\n",
       "      <th>abductive_nli_8</th>\n",
       "      <th>...</th>\n",
       "      <th>wsc_42</th>\n",
       "      <th>wsc_43</th>\n",
       "      <th>wsc_44</th>\n",
       "      <th>wsc_45</th>\n",
       "      <th>wsc_46</th>\n",
       "      <th>wsc_47</th>\n",
       "      <th>wsc_48</th>\n",
       "      <th>wsc_49</th>\n",
       "      <th>wsc_50</th>\n",
       "      <th>wsc_51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-base-10M-1_best</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-10M-1_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roberta-base-10M-1_25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roberta-base-10M-1_50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base-10M-1_10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>xlm-roberta-large_best</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>xlm-roberta-large_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>xlm-roberta-large_25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>xlm-roberta-large_50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>xlm-roberta-large_10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 79349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     userid  abductive_nli_0  abductive_nli_1  \\\n",
       "0   roberta-base-10M-1_best                1                0   \n",
       "1      roberta-base-10M-1_1                0                0   \n",
       "2     roberta-base-10M-1_25                1                1   \n",
       "3     roberta-base-10M-1_50                1                0   \n",
       "4     roberta-base-10M-1_10                1                0   \n",
       "..                      ...              ...              ...   \n",
       "85   xlm-roberta-large_best                1                1   \n",
       "86      xlm-roberta-large_1                1                1   \n",
       "87     xlm-roberta-large_25                1                1   \n",
       "88     xlm-roberta-large_50                1                1   \n",
       "89     xlm-roberta-large_10                0                1   \n",
       "\n",
       "    abductive_nli_2  abductive_nli_3  abductive_nli_4  abductive_nli_5  \\\n",
       "0                 1                1                1                1   \n",
       "1                 0                1                1                0   \n",
       "2                 1                1                1                1   \n",
       "3                 1                1                1                1   \n",
       "4                 1                1                1                1   \n",
       "..              ...              ...              ...              ...   \n",
       "85                0                1                1                1   \n",
       "86                0                1                1                0   \n",
       "87                0                1                1                1   \n",
       "88                1                1                1                1   \n",
       "89                0                1                1                0   \n",
       "\n",
       "    abductive_nli_6  abductive_nli_7  abductive_nli_8  ...  wsc_42  wsc_43  \\\n",
       "0                 1                0                1  ...       1       0   \n",
       "1                 1                0                1  ...       1       0   \n",
       "2                 1                1                1  ...       0       0   \n",
       "3                 1                0                1  ...       1       0   \n",
       "4                 1                0                1  ...       1       0   \n",
       "..              ...              ...              ...  ...     ...     ...   \n",
       "85                1                1                1  ...       1       1   \n",
       "86                1                0                1  ...       0       0   \n",
       "87                0                1                1  ...       1       1   \n",
       "88                1                1                1  ...       1       1   \n",
       "89                0                1                1  ...       0       0   \n",
       "\n",
       "    wsc_44  wsc_45  wsc_46  wsc_47  wsc_48  wsc_49  wsc_50  wsc_51  \n",
       "0        1       1       0       1       0       1       0       1  \n",
       "1        1       0       0       1       0       0       0       1  \n",
       "2        0       1       0       1       1       1       1       1  \n",
       "3        0       1       0       1       1       1       0       1  \n",
       "4        0       1       0       1       1       1       0       1  \n",
       "..     ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "85       0       1       0       1       1       1       0       0  \n",
       "86       1       0       1       0       0       0       1       1  \n",
       "87       0       1       0       1       1       1       0       0  \n",
       "88       0       1       0       1       1       1       0       0  \n",
       "89       1       0       1       0       0       0       1       1  \n",
       "\n",
       "[90 rows x 79349 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "model_levels = []\n",
    "for m in combined_responses['userid']:\n",
    "    mname = m.split('_')[0]\n",
    "    mlevel = m.split('_')[-1]\n",
    "    if mname.endswith('-1') or mname.endswith('-2') or mname.endswith('-3'):\n",
    "        mname = mname[:-2]\n",
    "    model_names.append(mname)\n",
    "    \n",
    "    mlevel_append = '' if mlevel == 'best' else r'%'\n",
    "    model_levels.append(mlevel+mlevel_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "b 35641\n",
      "g 0\n",
      "t 47\n"
     ]
    }
   ],
   "source": [
    "#  we will only use log mean for discriminative parameter\n",
    "for param_key, param_stat in param_plot_stats_trimmed.items():\n",
    "    param_stat['log_mean'] = numpy.log(param_stat['mean'])\n",
    "    print(param_key, param_stat['log_mean'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_a = pd.concat([param_plot_stats_trimmed['a'], task_name_format_trimmed], axis=1)\n",
    "param_b = pd.concat([param_plot_stats_trimmed['b'], task_name_format_trimmed], axis=1)\n",
    "\n",
    "task_order = [task_metadata.loc[x]['taskname'] for x in task_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: albert-xxlarge-v2\n",
      "1.7490668296813965\n",
      "No gamma: 0.14549648163855539\n",
      "With gamma: 0.11648878309933183\n",
      "           mean\n",
      "0      0.156178\n",
      "1      0.208147\n",
      "2      0.069049\n",
      "3      0.014967\n",
      "4      0.102332\n",
      "...         ...\n",
      "79343  0.050886\n",
      "79344  0.110832\n",
      "79345  0.069405\n",
      "79346  0.167813\n",
      "79347  0.101622\n",
      "\n",
      "[79348 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "leh_scores = icc_best_deriv(\n",
    "    param_plot_stats_trimmed['a'],\n",
    "    param_plot_stats_trimmed['b'],\n",
    "    param_plot_stats_trimmed['t'],\n",
    "    model_names,\n",
    "    gamma = param_plot_stats_trimmed['g'],\n",
    ")\n",
    "\n",
    "leh_scores_plot = pd.DataFrame(pd.Series(leh_scores), columns = ['mean'])\n",
    "print(leh_scores_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leh_scores_plot_trimmed = pd.concat([leh_scores_plot, task_name_format_trimmed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metadata.set_index(\"taskname\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variational_irt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=\"boolq,cb,commonsenseqa,copa,cosmosqa,hellaswag,adversarial-nli,rte,snli,wic,qamr,arct,mcscript,mctaco,mutual,mutual-plus,quoref,socialiqa,squad-v2,wsc,mnli,mrqa-nq,newsqa,abductive-nli,arc-easy,arc-challenge,piqa,quail,winogrande,anli\"\n",
    "data_names, responses, n_items = get_files(\n",
    "    os.path.join(repo, 'data'),\n",
    "    \"csv\",\n",
    "    set(datasets.split(','))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metadata = pd.read_csv('task_metadata.csv')\n",
    "task_metadata.set_index(\"jiant_name\", inplace=True)\n",
    "task_list = [x for x in task_metadata.index if x in data_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "task_name = []\n",
    "task_format = []\n",
    "\n",
    "for tname, size in zip(data_names, n_items):\n",
    "    name = task_metadata.loc[tname]['taskname']\n",
    "    total += size\n",
    "    task_name += [name for _ in range(size)]\n",
    "    task_format += [task_metadata.loc[tname]['format'] for _ in range(size)]\n",
    "    \n",
    "task_name = pd.DataFrame(task_name, columns=['task_name'])\n",
    "task_format = pd.DataFrame(task_format, columns=['format'])\n",
    "task_name_format = pd.concat([task_name, task_format], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Params and Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = os.path.join(repo, 'params_trimmed_item', f'alpha-lognormal-identity_theta-normal-identity_nosubsample_1.00_0.30')\n",
    "p = 0.95\n",
    "\n",
    "with open(os.path.join(exp_dir, 'responses.p'), 'rb') as f:\n",
    "    combined_responses = pickle.load(f).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-large_best\n",
      "==========================================================================================\n",
      "Overall acc: 0.7661\n",
      "abductive_nli acc: 0.8564\n",
      "adversarial_nli acc: 0.4995\n",
      "arc_challenge acc: 0.3319\n",
      "arc_easy acc: 0.6299\n",
      "arct acc: 0.8604\n",
      "boolq acc: 0.8217\n",
      "cb acc: 0.8571\n",
      "commonsenseqa acc: 0.6759\n",
      "copa acc: 0.8400\n",
      "cosmosqa acc: 0.8000\n",
      "hellaswag acc: 0.8420\n",
      "mcscript acc: 0.9183\n",
      "mctaco acc: 0.6010\n",
      "mnli acc: 0.8995\n",
      "mrqa_natural_questions acc: 0.7489\n",
      "mutual_plus acc: 0.7314\n",
      "mutual acc: 0.8668\n",
      "newsqa acc: 0.6608\n",
      "piqa acc: 0.7617\n",
      "qamr acc: 0.7944\n",
      "quail acc: 0.6691\n",
      "quoref acc: 0.8241\n",
      "rte acc: 0.8345\n",
      "snli acc: 0.9192\n",
      "socialiqa acc: 0.7738\n",
      "squad_v2 acc: 0.4337\n",
      "wic acc: 0.7085\n",
      "winogrande acc: 0.7697\n",
      "wsc acc: 0.6154\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy of roberta-large models\n",
    "\n",
    "extractmodel = 'roberta-large_best'\n",
    "tie_break = 0\n",
    "\n",
    "acc_by_dataset = {}\n",
    "\n",
    "roberta_rp = combined_responses.loc[combined_responses['userid']==extractmodel, :]\n",
    "if roberta_rp.shape[0] > 1:\n",
    "    roberta_rp = roberta_rp.iloc[tie_break, :]\n",
    "\n",
    "cols = combined_responses.columns.values\n",
    "\n",
    "for item in cols[1:]:\n",
    "    data_name = '_'.join(item.split('_')[:-1])\n",
    "    resp = roberta_rp[item].item()\n",
    "    \n",
    "    if data_name in acc_by_dataset:\n",
    "        acc_by_dataset[data_name]['correct'] += resp\n",
    "        acc_by_dataset[data_name]['total'] += 1\n",
    "    else:\n",
    "        acc_by_dataset[data_name] = {'correct': resp, 'total': 1}\n",
    "\n",
    "print(extractmodel)\n",
    "print('='*90)\n",
    "print(f'Overall acc: {roberta_rp.iloc[0, 1:].sum()/(roberta_rp.shape[1]-1):.4f}')        \n",
    "\n",
    "for data_name, acc_dict in acc_by_dataset.items():\n",
    "    print(f'{data_name} acc: {acc_dict[\"correct\"]/acc_dict[\"total\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to False if run for the first time\n",
    "# note that this will take sometimes to run if the datasets are big\n",
    "load_from_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution and transformation\n",
    "alpha_dist = 'log-normal'\n",
    "alpha_transf = 'standard'\n",
    "theta_dist = 'normal'\n",
    "theta_transf = 'standard'\n",
    "\n",
    "exp_dir = os.path.join(repo, 'params', f'alpha-lognormal-identity_theta-normal-identity_nosubsample_1.00_0.30')\n",
    "p = 0.95\n",
    "\n",
    "with open(os.path.join(exp_dir, 'responses.p'), 'rb') as f:\n",
    "    combined_responses = pickle.load(f).reset_index()\n",
    "data, accuracies, example_accuracies = get_data_accuracies(combined_responses)\n",
    "column_names = combined_responses.columns[1:]\n",
    "select_ts = {\n",
    "    'standard':lambda x:x,\n",
    "    'positive':lambda x:torch.log(1+torch.exp(torch.tensor(x))),\n",
    "    'sigmoid':lambda x:sigmoid(torch.tensor(x)),\n",
    "}\n",
    "\n",
    "transforms = {\n",
    "    'a':select_ts[alpha_transf],\n",
    "    'b':select_ts['standard'],\n",
    "    'g':select_ts['sigmoid'],\n",
    "    't':select_ts[theta_transf],\n",
    "}\n",
    "\n",
    "if load_from_cache:\n",
    "    param_plot_stats = {}\n",
    "\n",
    "    for key in transforms.keys():\n",
    "        with open(os.path.join('plot_stats_pickles', f'{key}.p'), 'rb') as f:\n",
    "            param_plot_stats[key] = pickle.load(f)\n",
    "else:\n",
    "    param_plot_stats = get_plot_stats(\n",
    "        exp_dir,\n",
    "        alpha_dist,\n",
    "        theta_dist,\n",
    "        transforms,\n",
    "        p = 0.95\n",
    "    )\n",
    "    \n",
    "    os.makedirs('plot_stats_pickles', exist_ok=True)\n",
    "    for key, value in param_plot_stats.items():\n",
    "        with open(os.path.join('plot_stats_pickles', f'{key}.p'), 'wb') as f:\n",
    "            pickle.dump(value, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>abductive_nli_0</th>\n",
       "      <th>abductive_nli_1</th>\n",
       "      <th>abductive_nli_2</th>\n",
       "      <th>abductive_nli_3</th>\n",
       "      <th>abductive_nli_4</th>\n",
       "      <th>abductive_nli_5</th>\n",
       "      <th>abductive_nli_6</th>\n",
       "      <th>abductive_nli_7</th>\n",
       "      <th>abductive_nli_8</th>\n",
       "      <th>...</th>\n",
       "      <th>wsc_42</th>\n",
       "      <th>wsc_43</th>\n",
       "      <th>wsc_44</th>\n",
       "      <th>wsc_45</th>\n",
       "      <th>wsc_46</th>\n",
       "      <th>wsc_47</th>\n",
       "      <th>wsc_48</th>\n",
       "      <th>wsc_49</th>\n",
       "      <th>wsc_50</th>\n",
       "      <th>wsc_51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-base-10M-1_best</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base-10M-1_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roberta-base-10M-1_25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roberta-base-10M-1_50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta-base-10M-1_10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>xlm-roberta-large_best</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>xlm-roberta-large_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>xlm-roberta-large_25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>xlm-roberta-large_50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>xlm-roberta-large_10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 82757 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     userid  abductive_nli_0  abductive_nli_1  \\\n",
       "0   roberta-base-10M-1_best                1                0   \n",
       "1      roberta-base-10M-1_1                0                0   \n",
       "2     roberta-base-10M-1_25                1                1   \n",
       "3     roberta-base-10M-1_50                1                0   \n",
       "4     roberta-base-10M-1_10                1                0   \n",
       "..                      ...              ...              ...   \n",
       "85   xlm-roberta-large_best                1                1   \n",
       "86      xlm-roberta-large_1                1                1   \n",
       "87     xlm-roberta-large_25                1                1   \n",
       "88     xlm-roberta-large_50                1                1   \n",
       "89     xlm-roberta-large_10                0                1   \n",
       "\n",
       "    abductive_nli_2  abductive_nli_3  abductive_nli_4  abductive_nli_5  \\\n",
       "0                 1                1                1                1   \n",
       "1                 0                1                1                0   \n",
       "2                 1                1                1                1   \n",
       "3                 1                1                1                1   \n",
       "4                 1                1                1                1   \n",
       "..              ...              ...              ...              ...   \n",
       "85                0                1                1                1   \n",
       "86                0                1                1                0   \n",
       "87                0                1                1                1   \n",
       "88                1                1                1                1   \n",
       "89                0                1                1                0   \n",
       "\n",
       "    abductive_nli_6  abductive_nli_7  abductive_nli_8  ...  wsc_42  wsc_43  \\\n",
       "0                 1                0                1  ...       1       0   \n",
       "1                 1                0                1  ...       1       0   \n",
       "2                 1                1                1  ...       0       0   \n",
       "3                 1                0                1  ...       1       0   \n",
       "4                 1                0                1  ...       1       0   \n",
       "..              ...              ...              ...  ...     ...     ...   \n",
       "85                1                1                1  ...       1       1   \n",
       "86                1                0                1  ...       0       0   \n",
       "87                0                1                1  ...       1       1   \n",
       "88                1                1                1  ...       1       1   \n",
       "89                0                1                1  ...       0       0   \n",
       "\n",
       "    wsc_44  wsc_45  wsc_46  wsc_47  wsc_48  wsc_49  wsc_50  wsc_51  \n",
       "0        1       1       0       1       0       1       0       1  \n",
       "1        1       0       0       1       0       0       0       1  \n",
       "2        0       1       0       1       1       1       1       1  \n",
       "3        0       1       0       1       1       1       0       1  \n",
       "4        0       1       0       1       1       1       0       1  \n",
       "..     ...     ...     ...     ...     ...     ...     ...     ...  \n",
       "85       0       1       0       1       1       1       0       0  \n",
       "86       1       0       1       0       0       0       1       1  \n",
       "87       0       1       0       1       1       1       0       0  \n",
       "88       0       1       0       1       1       1       0       0  \n",
       "89       1       0       1       0       0       0       1       1  \n",
       "\n",
       "[90 rows x 82757 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "model_levels = []\n",
    "for m in combined_responses['userid']:\n",
    "    mname = m.split('_')[0]\n",
    "    mlevel = m.split('_')[-1]\n",
    "    if mname.endswith('-1') or mname.endswith('-2') or mname.endswith('-3'):\n",
    "        mname = mname[:-2]\n",
    "    model_names.append(mname)\n",
    "    \n",
    "    mlevel_append = '' if mlevel == 'best' else r'%'\n",
    "    model_levels.append(mlevel+mlevel_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "b 35716\n",
      "g 0\n",
      "t 47\n"
     ]
    }
   ],
   "source": [
    "#  we will only use log mean for discriminative parameter\n",
    "for param_key, param_stat in param_plot_stats.items():\n",
    "    param_stat['log_mean'] = numpy.log(param_stat['mean'])\n",
    "    print(param_key, param_stat['log_mean'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_a = pd.concat([param_plot_stats['a'], task_name_format], axis=1)\n",
    "param_b = pd.concat([param_plot_stats['b'], task_name_format], axis=1)\n",
    "\n",
    "task_order = [task_metadata.loc[x]['taskname'] for x in task_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: albert-xxlarge-v2\n",
      "1.6105766296386719\n",
      "No gamma: 0.14762448462619243\n",
      "With gamma: 0.1185372240065298\n",
      "           mean\n",
      "0      0.171294\n",
      "1      0.230789\n",
      "2      0.064061\n",
      "3      0.015200\n",
      "4      0.090730\n",
      "...         ...\n",
      "82751  0.059779\n",
      "82752  0.141603\n",
      "82753  0.091485\n",
      "82754  0.128162\n",
      "82755  0.097277\n",
      "\n",
      "[82756 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "leh_scores = icc_best_deriv(\n",
    "    param_plot_stats['a'],\n",
    "    param_plot_stats['b'],\n",
    "    param_plot_stats['t'],\n",
    "    model_names,\n",
    "    gamma = param_plot_stats['g'],\n",
    ")\n",
    "\n",
    "leh_scores_plot = pd.DataFrame(pd.Series(leh_scores), columns = ['mean'])\n",
    "print(leh_scores_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leh_scores_plot = pd.concat([leh_scores_plot, task_name_format], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_metadata.set_index(\"taskname\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trimmed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.210905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-C</th>\n",
       "      <td>0.231316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-E</th>\n",
       "      <td>0.220743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCT</th>\n",
       "      <td>0.144571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AbductNLI</th>\n",
       "      <td>0.184625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.141349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB</th>\n",
       "      <td>0.108294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COPA</th>\n",
       "      <td>0.181559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSQA</th>\n",
       "      <td>0.235250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CosmosQA</th>\n",
       "      <td>0.205929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.256265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC-TACO</th>\n",
       "      <td>0.270738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCScript</th>\n",
       "      <td>0.129385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNLI</th>\n",
       "      <td>0.114458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRQA-NQ</th>\n",
       "      <td>0.200532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual</th>\n",
       "      <td>0.213936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual+</th>\n",
       "      <td>0.217378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewsQA</th>\n",
       "      <td>0.257289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PiQA</th>\n",
       "      <td>0.164410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QAMR</th>\n",
       "      <td>0.189043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuAIL</th>\n",
       "      <td>0.239120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quoref</th>\n",
       "      <td>0.260533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTE</th>\n",
       "      <td>0.165325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNLI</th>\n",
       "      <td>0.045599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialIQA</th>\n",
       "      <td>0.200333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSC</th>\n",
       "      <td>0.233525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WiC</th>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>0.200612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trimmed\n",
       "task_name           \n",
       "ANLI        0.210905\n",
       "ARC-C       0.231316\n",
       "ARC-E       0.220743\n",
       "ARCT        0.144571\n",
       "AbductNLI   0.184625\n",
       "BoolQ       0.141349\n",
       "CB          0.108294\n",
       "COPA        0.181559\n",
       "CSQA        0.235250\n",
       "CosmosQA    0.205929\n",
       "HellaSwag   0.256265\n",
       "MC-TACO     0.270738\n",
       "MCScript    0.129385\n",
       "MNLI        0.114458\n",
       "MRQA-NQ     0.200532\n",
       "MuTual      0.213936\n",
       "MuTual+     0.217378\n",
       "NewsQA      0.257289\n",
       "PiQA        0.164410\n",
       "QAMR        0.189043\n",
       "QuAIL       0.239120\n",
       "Quoref      0.260533\n",
       "RTE         0.165325\n",
       "SNLI        0.045599\n",
       "SocialIQA   0.200333\n",
       "WSC         0.233525\n",
       "WiC         0.245300\n",
       "Winogrande  0.200612"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed = leh_scores_plot_trimmed.groupby(by='task_name').quantile(q=0.75).rename(columns={'mean':'Trimmed'})\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.202093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-C</th>\n",
       "      <td>0.224374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-E</th>\n",
       "      <td>0.221798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCT</th>\n",
       "      <td>0.146663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AbductNLI</th>\n",
       "      <td>0.180351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.126232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB</th>\n",
       "      <td>0.102626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COPA</th>\n",
       "      <td>0.173248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSQA</th>\n",
       "      <td>0.239099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CosmosQA</th>\n",
       "      <td>0.204523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.259556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC-TACO</th>\n",
       "      <td>0.256297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCScript</th>\n",
       "      <td>0.135480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNLI</th>\n",
       "      <td>0.120850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRQA-NQ</th>\n",
       "      <td>0.188073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual</th>\n",
       "      <td>0.207742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual+</th>\n",
       "      <td>0.215006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewsQA</th>\n",
       "      <td>0.245779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PiQA</th>\n",
       "      <td>0.163360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QAMR</th>\n",
       "      <td>0.178407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuAIL</th>\n",
       "      <td>0.236739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quoref</th>\n",
       "      <td>0.260933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTE</th>\n",
       "      <td>0.180724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNLI</th>\n",
       "      <td>0.050806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialIQA</th>\n",
       "      <td>0.200835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSC</th>\n",
       "      <td>0.224870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WiC</th>\n",
       "      <td>0.255731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>0.210921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Full\n",
       "task_name           \n",
       "ANLI        0.202093\n",
       "ARC-C       0.224374\n",
       "ARC-E       0.221798\n",
       "ARCT        0.146663\n",
       "AbductNLI   0.180351\n",
       "BoolQ       0.126232\n",
       "CB          0.102626\n",
       "COPA        0.173248\n",
       "CSQA        0.239099\n",
       "CosmosQA    0.204523\n",
       "HellaSwag   0.259556\n",
       "MC-TACO     0.256297\n",
       "MCScript    0.135480\n",
       "MNLI        0.120850\n",
       "MRQA-NQ     0.188073\n",
       "MuTual      0.207742\n",
       "MuTual+     0.215006\n",
       "NewsQA      0.245779\n",
       "PiQA        0.163360\n",
       "QAMR        0.178407\n",
       "QuAIL       0.236739\n",
       "Quoref      0.260933\n",
       "RTE         0.180724\n",
       "SNLI        0.050806\n",
       "SocialIQA   0.200835\n",
       "WSC         0.224870\n",
       "WiC         0.255731\n",
       "Winogrande  0.210921"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = leh_scores_plot.groupby(by='task_name').quantile(q=0.75).rename(columns={'mean':'Full'})\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median diff (magnitude) 0.006144598435523396\n",
      "standard deviation diff (magnitude) 0.004611184132519173\n",
      "median rel diff (magnitude) 0.039637497671869554\n"
     ]
    }
   ],
   "source": [
    "combined = pd.concat([trimmed, full], axis=1)\n",
    "combined['diff'] = combined['Trimmed'] - combined['Full']\n",
    "combined['rel_diff'] = (combined['Trimmed'] - combined['Full'])/combined['Full']\n",
    "\n",
    "print('median diff (magnitude)', combined['diff'].abs().median())\n",
    "print('standard deviation diff (magnitude)', combined['diff'].abs().std())\n",
    "print('median rel diff (magnitude)', combined['rel_diff'].abs().median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trimmed</th>\n",
       "      <th>Full</th>\n",
       "      <th>diff</th>\n",
       "      <th>rel_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RTE</th>\n",
       "      <td>0.165325</td>\n",
       "      <td>0.180724</td>\n",
       "      <td>-0.015399</td>\n",
       "      <td>-0.085208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WiC</th>\n",
       "      <td>0.245300</td>\n",
       "      <td>0.255731</td>\n",
       "      <td>-0.010431</td>\n",
       "      <td>-0.040788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>0.200612</td>\n",
       "      <td>0.210921</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>-0.048874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNLI</th>\n",
       "      <td>0.114458</td>\n",
       "      <td>0.120850</td>\n",
       "      <td>-0.006393</td>\n",
       "      <td>-0.052899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCScript</th>\n",
       "      <td>0.129385</td>\n",
       "      <td>0.135480</td>\n",
       "      <td>-0.006095</td>\n",
       "      <td>-0.044990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNLI</th>\n",
       "      <td>0.045599</td>\n",
       "      <td>0.050806</td>\n",
       "      <td>-0.005207</td>\n",
       "      <td>-0.102493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSQA</th>\n",
       "      <td>0.235250</td>\n",
       "      <td>0.239099</td>\n",
       "      <td>-0.003849</td>\n",
       "      <td>-0.016097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.256265</td>\n",
       "      <td>0.259556</td>\n",
       "      <td>-0.003291</td>\n",
       "      <td>-0.012679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCT</th>\n",
       "      <td>0.144571</td>\n",
       "      <td>0.146663</td>\n",
       "      <td>-0.002092</td>\n",
       "      <td>-0.014264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-E</th>\n",
       "      <td>0.220743</td>\n",
       "      <td>0.221798</td>\n",
       "      <td>-0.001055</td>\n",
       "      <td>-0.004756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialIQA</th>\n",
       "      <td>0.200333</td>\n",
       "      <td>0.200835</td>\n",
       "      <td>-0.000503</td>\n",
       "      <td>-0.002503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quoref</th>\n",
       "      <td>0.260533</td>\n",
       "      <td>0.260933</td>\n",
       "      <td>-0.000400</td>\n",
       "      <td>-0.001532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PiQA</th>\n",
       "      <td>0.164410</td>\n",
       "      <td>0.163360</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.006431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CosmosQA</th>\n",
       "      <td>0.205929</td>\n",
       "      <td>0.204523</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.006875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual+</th>\n",
       "      <td>0.217378</td>\n",
       "      <td>0.215006</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>0.011032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuAIL</th>\n",
       "      <td>0.239120</td>\n",
       "      <td>0.236739</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.010057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AbductNLI</th>\n",
       "      <td>0.184625</td>\n",
       "      <td>0.180351</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB</th>\n",
       "      <td>0.108294</td>\n",
       "      <td>0.102626</td>\n",
       "      <td>0.005667</td>\n",
       "      <td>0.055223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual</th>\n",
       "      <td>0.213936</td>\n",
       "      <td>0.207742</td>\n",
       "      <td>0.006194</td>\n",
       "      <td>0.029816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-C</th>\n",
       "      <td>0.231316</td>\n",
       "      <td>0.224374</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.030937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COPA</th>\n",
       "      <td>0.181559</td>\n",
       "      <td>0.173248</td>\n",
       "      <td>0.008311</td>\n",
       "      <td>0.047971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSC</th>\n",
       "      <td>0.233525</td>\n",
       "      <td>0.224870</td>\n",
       "      <td>0.008655</td>\n",
       "      <td>0.038487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.210905</td>\n",
       "      <td>0.202093</td>\n",
       "      <td>0.008812</td>\n",
       "      <td>0.043603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QAMR</th>\n",
       "      <td>0.189043</td>\n",
       "      <td>0.178407</td>\n",
       "      <td>0.010636</td>\n",
       "      <td>0.059618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewsQA</th>\n",
       "      <td>0.257289</td>\n",
       "      <td>0.245779</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>0.046832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRQA-NQ</th>\n",
       "      <td>0.200532</td>\n",
       "      <td>0.188073</td>\n",
       "      <td>0.012458</td>\n",
       "      <td>0.066241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC-TACO</th>\n",
       "      <td>0.270738</td>\n",
       "      <td>0.256297</td>\n",
       "      <td>0.014441</td>\n",
       "      <td>0.056346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.141349</td>\n",
       "      <td>0.126232</td>\n",
       "      <td>0.015118</td>\n",
       "      <td>0.119761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trimmed      Full      diff  rel_diff\n",
       "task_name                                         \n",
       "RTE         0.165325  0.180724 -0.015399 -0.085208\n",
       "WiC         0.245300  0.255731 -0.010431 -0.040788\n",
       "Winogrande  0.200612  0.210921 -0.010309 -0.048874\n",
       "MNLI        0.114458  0.120850 -0.006393 -0.052899\n",
       "MCScript    0.129385  0.135480 -0.006095 -0.044990\n",
       "SNLI        0.045599  0.050806 -0.005207 -0.102493\n",
       "CSQA        0.235250  0.239099 -0.003849 -0.016097\n",
       "HellaSwag   0.256265  0.259556 -0.003291 -0.012679\n",
       "ARCT        0.144571  0.146663 -0.002092 -0.014264\n",
       "ARC-E       0.220743  0.221798 -0.001055 -0.004756\n",
       "SocialIQA   0.200333  0.200835 -0.000503 -0.002503\n",
       "Quoref      0.260533  0.260933 -0.000400 -0.001532\n",
       "PiQA        0.164410  0.163360  0.001051  0.006431\n",
       "CosmosQA    0.205929  0.204523  0.001406  0.006875\n",
       "MuTual+     0.217378  0.215006  0.002372  0.011032\n",
       "QuAIL       0.239120  0.236739  0.002381  0.010057\n",
       "AbductNLI   0.184625  0.180351  0.004274  0.023700\n",
       "CB          0.108294  0.102626  0.005667  0.055223\n",
       "MuTual      0.213936  0.207742  0.006194  0.029816\n",
       "ARC-C       0.231316  0.224374  0.006941  0.030937\n",
       "COPA        0.181559  0.173248  0.008311  0.047971\n",
       "WSC         0.233525  0.224870  0.008655  0.038487\n",
       "ANLI        0.210905  0.202093  0.008812  0.043603\n",
       "QAMR        0.189043  0.178407  0.010636  0.059618\n",
       "NewsQA      0.257289  0.245779  0.011510  0.046832\n",
       "MRQA-NQ     0.200532  0.188073  0.012458  0.066241\n",
       "MC-TACO     0.270738  0.256297  0.014441  0.056346\n",
       "BoolQ       0.141349  0.126232  0.015118  0.119761"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.loc[:,['Trimmed', 'Full', 'diff', 'rel_diff']].sort_values(by='diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trimmed</th>\n",
       "      <th>Full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trimmed</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Full</th>\n",
       "      <td>0.988994</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Trimmed      Full\n",
       "Trimmed  1.000000  0.988994\n",
       "Full     0.988994  1.000000"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.loc[:,['Trimmed','Full']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79348, 5)\n"
     ]
    }
   ],
   "source": [
    "print(param_plot_stats_trimmed['a'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73167, 2)\n"
     ]
    }
   ],
   "source": [
    "print(task_name_format_trimmed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>Trimmed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.899919</td>\n",
       "      <td>2.226639</td>\n",
       "      <td>1.441112</td>\n",
       "      <td>0.125407</td>\n",
       "      <td>0.365415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-C</th>\n",
       "      <td>0.822956</td>\n",
       "      <td>2.078902</td>\n",
       "      <td>1.324271</td>\n",
       "      <td>0.109164</td>\n",
       "      <td>0.280862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-E</th>\n",
       "      <td>0.831715</td>\n",
       "      <td>2.070082</td>\n",
       "      <td>1.332630</td>\n",
       "      <td>0.109028</td>\n",
       "      <td>0.287154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCT</th>\n",
       "      <td>0.792842</td>\n",
       "      <td>2.039686</td>\n",
       "      <td>1.283021</td>\n",
       "      <td>0.118357</td>\n",
       "      <td>0.249218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AbductNLI</th>\n",
       "      <td>0.824605</td>\n",
       "      <td>2.203403</td>\n",
       "      <td>1.362311</td>\n",
       "      <td>0.135256</td>\n",
       "      <td>0.309183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.830674</td>\n",
       "      <td>2.135282</td>\n",
       "      <td>1.341990</td>\n",
       "      <td>0.120325</td>\n",
       "      <td>0.294153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB</th>\n",
       "      <td>0.902353</td>\n",
       "      <td>2.114346</td>\n",
       "      <td>1.383875</td>\n",
       "      <td>0.096450</td>\n",
       "      <td>0.324880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COPA</th>\n",
       "      <td>0.715233</td>\n",
       "      <td>2.144357</td>\n",
       "      <td>1.261290</td>\n",
       "      <td>0.139823</td>\n",
       "      <td>0.232093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSQA</th>\n",
       "      <td>0.870722</td>\n",
       "      <td>2.138904</td>\n",
       "      <td>1.376232</td>\n",
       "      <td>0.114204</td>\n",
       "      <td>0.319349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CosmosQA</th>\n",
       "      <td>0.872706</td>\n",
       "      <td>2.162746</td>\n",
       "      <td>1.391881</td>\n",
       "      <td>0.117798</td>\n",
       "      <td>0.330656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.872078</td>\n",
       "      <td>2.157211</td>\n",
       "      <td>1.390186</td>\n",
       "      <td>0.118538</td>\n",
       "      <td>0.329438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC-TACO</th>\n",
       "      <td>0.977371</td>\n",
       "      <td>2.210478</td>\n",
       "      <td>1.491807</td>\n",
       "      <td>0.117288</td>\n",
       "      <td>0.399988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCScript</th>\n",
       "      <td>0.824083</td>\n",
       "      <td>2.131228</td>\n",
       "      <td>1.349454</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>0.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNLI</th>\n",
       "      <td>0.947808</td>\n",
       "      <td>2.331863</td>\n",
       "      <td>1.506464</td>\n",
       "      <td>0.136649</td>\n",
       "      <td>0.409765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRQA-NQ</th>\n",
       "      <td>1.084805</td>\n",
       "      <td>2.484006</td>\n",
       "      <td>1.660469</td>\n",
       "      <td>0.139069</td>\n",
       "      <td>0.507100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual</th>\n",
       "      <td>0.862730</td>\n",
       "      <td>2.131898</td>\n",
       "      <td>1.370202</td>\n",
       "      <td>0.109107</td>\n",
       "      <td>0.314958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual+</th>\n",
       "      <td>0.878754</td>\n",
       "      <td>2.182751</td>\n",
       "      <td>1.423695</td>\n",
       "      <td>0.121227</td>\n",
       "      <td>0.353255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewsQA</th>\n",
       "      <td>1.081411</td>\n",
       "      <td>2.419944</td>\n",
       "      <td>1.632212</td>\n",
       "      <td>0.127042</td>\n",
       "      <td>0.489936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PiQA</th>\n",
       "      <td>0.709615</td>\n",
       "      <td>1.943208</td>\n",
       "      <td>1.197718</td>\n",
       "      <td>0.107507</td>\n",
       "      <td>0.180418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QAMR</th>\n",
       "      <td>1.203754</td>\n",
       "      <td>2.691427</td>\n",
       "      <td>1.833569</td>\n",
       "      <td>0.154994</td>\n",
       "      <td>0.606264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuAIL</th>\n",
       "      <td>0.896824</td>\n",
       "      <td>2.174109</td>\n",
       "      <td>1.425712</td>\n",
       "      <td>0.116587</td>\n",
       "      <td>0.354671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quoref</th>\n",
       "      <td>1.071343</td>\n",
       "      <td>2.378822</td>\n",
       "      <td>1.616865</td>\n",
       "      <td>0.120886</td>\n",
       "      <td>0.480489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTE</th>\n",
       "      <td>0.675737</td>\n",
       "      <td>1.948719</td>\n",
       "      <td>1.167012</td>\n",
       "      <td>0.112650</td>\n",
       "      <td>0.154438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNLI</th>\n",
       "      <td>0.971650</td>\n",
       "      <td>2.382312</td>\n",
       "      <td>1.538591</td>\n",
       "      <td>0.145453</td>\n",
       "      <td>0.430867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialIQA</th>\n",
       "      <td>0.834107</td>\n",
       "      <td>2.144129</td>\n",
       "      <td>1.357319</td>\n",
       "      <td>0.119848</td>\n",
       "      <td>0.305512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSC</th>\n",
       "      <td>1.045797</td>\n",
       "      <td>2.420328</td>\n",
       "      <td>1.651690</td>\n",
       "      <td>0.126998</td>\n",
       "      <td>0.501799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WiC</th>\n",
       "      <td>1.028403</td>\n",
       "      <td>2.319927</td>\n",
       "      <td>1.568458</td>\n",
       "      <td>0.118006</td>\n",
       "      <td>0.450093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>1.057519</td>\n",
       "      <td>2.367742</td>\n",
       "      <td>1.597664</td>\n",
       "      <td>0.122388</td>\n",
       "      <td>0.468542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lower     upper      mean       var   Trimmed\n",
       "task_name                                                   \n",
       "ANLI        0.899919  2.226639  1.441112  0.125407  0.365415\n",
       "ARC-C       0.822956  2.078902  1.324271  0.109164  0.280862\n",
       "ARC-E       0.831715  2.070082  1.332630  0.109028  0.287154\n",
       "ARCT        0.792842  2.039686  1.283021  0.118357  0.249218\n",
       "AbductNLI   0.824605  2.203403  1.362311  0.135256  0.309183\n",
       "BoolQ       0.830674  2.135282  1.341990  0.120325  0.294153\n",
       "CB          0.902353  2.114346  1.383875  0.096450  0.324880\n",
       "COPA        0.715233  2.144357  1.261290  0.139823  0.232093\n",
       "CSQA        0.870722  2.138904  1.376232  0.114204  0.319349\n",
       "CosmosQA    0.872706  2.162746  1.391881  0.117798  0.330656\n",
       "HellaSwag   0.872078  2.157211  1.390186  0.118538  0.329438\n",
       "MC-TACO     0.977371  2.210478  1.491807  0.117288  0.399988\n",
       "MCScript    0.824083  2.131228  1.349454  0.123718  0.299700\n",
       "MNLI        0.947808  2.331863  1.506464  0.136649  0.409765\n",
       "MRQA-NQ     1.084805  2.484006  1.660469  0.139069  0.507100\n",
       "MuTual      0.862730  2.131898  1.370202  0.109107  0.314958\n",
       "MuTual+     0.878754  2.182751  1.423695  0.121227  0.353255\n",
       "NewsQA      1.081411  2.419944  1.632212  0.127042  0.489936\n",
       "PiQA        0.709615  1.943208  1.197718  0.107507  0.180418\n",
       "QAMR        1.203754  2.691427  1.833569  0.154994  0.606264\n",
       "QuAIL       0.896824  2.174109  1.425712  0.116587  0.354671\n",
       "Quoref      1.071343  2.378822  1.616865  0.120886  0.480489\n",
       "RTE         0.675737  1.948719  1.167012  0.112650  0.154438\n",
       "SNLI        0.971650  2.382312  1.538591  0.145453  0.430867\n",
       "SocialIQA   0.834107  2.144129  1.357319  0.119848  0.305512\n",
       "WSC         1.045797  2.420328  1.651690  0.126998  0.501799\n",
       "WiC         1.028403  2.319927  1.568458  0.118006  0.450093\n",
       "Winogrande  1.057519  2.367742  1.597664  0.122388  0.468542"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed = pd.concat([param_plot_stats_trimmed['a'], task_name_format_trimmed], axis=1) \n",
    "trimmed = trimmed.groupby(by='task_name').quantile(q=0.75).rename(columns={'log_mean':'Trimmed'})\n",
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>Full</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.915775</td>\n",
       "      <td>2.265168</td>\n",
       "      <td>1.467177</td>\n",
       "      <td>0.128683</td>\n",
       "      <td>0.383340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-C</th>\n",
       "      <td>0.831736</td>\n",
       "      <td>2.101384</td>\n",
       "      <td>1.334468</td>\n",
       "      <td>0.114224</td>\n",
       "      <td>0.288532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-E</th>\n",
       "      <td>0.838473</td>\n",
       "      <td>2.104504</td>\n",
       "      <td>1.347662</td>\n",
       "      <td>0.113631</td>\n",
       "      <td>0.298371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCT</th>\n",
       "      <td>0.788995</td>\n",
       "      <td>2.132945</td>\n",
       "      <td>1.314351</td>\n",
       "      <td>0.120544</td>\n",
       "      <td>0.273343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AbductNLI</th>\n",
       "      <td>0.840363</td>\n",
       "      <td>2.236310</td>\n",
       "      <td>1.391650</td>\n",
       "      <td>0.141433</td>\n",
       "      <td>0.330490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.797130</td>\n",
       "      <td>2.123107</td>\n",
       "      <td>1.297305</td>\n",
       "      <td>0.124242</td>\n",
       "      <td>0.260289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB</th>\n",
       "      <td>0.892566</td>\n",
       "      <td>2.254348</td>\n",
       "      <td>1.444007</td>\n",
       "      <td>0.125014</td>\n",
       "      <td>0.367416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COPA</th>\n",
       "      <td>0.691717</td>\n",
       "      <td>2.040448</td>\n",
       "      <td>1.212194</td>\n",
       "      <td>0.122549</td>\n",
       "      <td>0.192426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSQA</th>\n",
       "      <td>0.865282</td>\n",
       "      <td>2.135357</td>\n",
       "      <td>1.386515</td>\n",
       "      <td>0.113656</td>\n",
       "      <td>0.326793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CosmosQA</th>\n",
       "      <td>0.876607</td>\n",
       "      <td>2.180389</td>\n",
       "      <td>1.398905</td>\n",
       "      <td>0.118627</td>\n",
       "      <td>0.335690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.874100</td>\n",
       "      <td>2.164195</td>\n",
       "      <td>1.402926</td>\n",
       "      <td>0.117966</td>\n",
       "      <td>0.338560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC-TACO</th>\n",
       "      <td>1.016922</td>\n",
       "      <td>2.337350</td>\n",
       "      <td>1.566093</td>\n",
       "      <td>0.125863</td>\n",
       "      <td>0.448584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCScript</th>\n",
       "      <td>0.824893</td>\n",
       "      <td>2.170641</td>\n",
       "      <td>1.364754</td>\n",
       "      <td>0.128255</td>\n",
       "      <td>0.310974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNLI</th>\n",
       "      <td>0.962192</td>\n",
       "      <td>2.378348</td>\n",
       "      <td>1.538096</td>\n",
       "      <td>0.141599</td>\n",
       "      <td>0.430545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRQA-NQ</th>\n",
       "      <td>1.095342</td>\n",
       "      <td>2.536280</td>\n",
       "      <td>1.693563</td>\n",
       "      <td>0.144988</td>\n",
       "      <td>0.526834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual</th>\n",
       "      <td>0.864732</td>\n",
       "      <td>2.128467</td>\n",
       "      <td>1.388061</td>\n",
       "      <td>0.109646</td>\n",
       "      <td>0.327908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual+</th>\n",
       "      <td>0.906054</td>\n",
       "      <td>2.184099</td>\n",
       "      <td>1.431886</td>\n",
       "      <td>0.118735</td>\n",
       "      <td>0.358992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewsQA</th>\n",
       "      <td>1.105347</td>\n",
       "      <td>2.496355</td>\n",
       "      <td>1.682510</td>\n",
       "      <td>0.137068</td>\n",
       "      <td>0.520287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PiQA</th>\n",
       "      <td>0.706009</td>\n",
       "      <td>1.955080</td>\n",
       "      <td>1.207704</td>\n",
       "      <td>0.111297</td>\n",
       "      <td>0.188721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QAMR</th>\n",
       "      <td>1.205043</td>\n",
       "      <td>2.712858</td>\n",
       "      <td>1.838317</td>\n",
       "      <td>0.158352</td>\n",
       "      <td>0.608850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuAIL</th>\n",
       "      <td>0.892183</td>\n",
       "      <td>2.226723</td>\n",
       "      <td>1.433457</td>\n",
       "      <td>0.124928</td>\n",
       "      <td>0.360089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quoref</th>\n",
       "      <td>1.079058</td>\n",
       "      <td>2.421550</td>\n",
       "      <td>1.641061</td>\n",
       "      <td>0.130273</td>\n",
       "      <td>0.495343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTE</th>\n",
       "      <td>0.713777</td>\n",
       "      <td>1.939449</td>\n",
       "      <td>1.196220</td>\n",
       "      <td>0.107657</td>\n",
       "      <td>0.179166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNLI</th>\n",
       "      <td>0.975155</td>\n",
       "      <td>2.431863</td>\n",
       "      <td>1.560219</td>\n",
       "      <td>0.153503</td>\n",
       "      <td>0.444826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialIQA</th>\n",
       "      <td>0.838094</td>\n",
       "      <td>2.105295</td>\n",
       "      <td>1.366151</td>\n",
       "      <td>0.114203</td>\n",
       "      <td>0.311997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSC</th>\n",
       "      <td>0.984332</td>\n",
       "      <td>2.236511</td>\n",
       "      <td>1.547790</td>\n",
       "      <td>0.101469</td>\n",
       "      <td>0.436810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WiC</th>\n",
       "      <td>1.020574</td>\n",
       "      <td>2.226973</td>\n",
       "      <td>1.536829</td>\n",
       "      <td>0.110745</td>\n",
       "      <td>0.429719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>1.005473</td>\n",
       "      <td>2.299273</td>\n",
       "      <td>1.544045</td>\n",
       "      <td>0.118759</td>\n",
       "      <td>0.434406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lower     upper      mean       var      Full\n",
       "task_name                                                   \n",
       "ANLI        0.915775  2.265168  1.467177  0.128683  0.383340\n",
       "ARC-C       0.831736  2.101384  1.334468  0.114224  0.288532\n",
       "ARC-E       0.838473  2.104504  1.347662  0.113631  0.298371\n",
       "ARCT        0.788995  2.132945  1.314351  0.120544  0.273343\n",
       "AbductNLI   0.840363  2.236310  1.391650  0.141433  0.330490\n",
       "BoolQ       0.797130  2.123107  1.297305  0.124242  0.260289\n",
       "CB          0.892566  2.254348  1.444007  0.125014  0.367416\n",
       "COPA        0.691717  2.040448  1.212194  0.122549  0.192426\n",
       "CSQA        0.865282  2.135357  1.386515  0.113656  0.326793\n",
       "CosmosQA    0.876607  2.180389  1.398905  0.118627  0.335690\n",
       "HellaSwag   0.874100  2.164195  1.402926  0.117966  0.338560\n",
       "MC-TACO     1.016922  2.337350  1.566093  0.125863  0.448584\n",
       "MCScript    0.824893  2.170641  1.364754  0.128255  0.310974\n",
       "MNLI        0.962192  2.378348  1.538096  0.141599  0.430545\n",
       "MRQA-NQ     1.095342  2.536280  1.693563  0.144988  0.526834\n",
       "MuTual      0.864732  2.128467  1.388061  0.109646  0.327908\n",
       "MuTual+     0.906054  2.184099  1.431886  0.118735  0.358992\n",
       "NewsQA      1.105347  2.496355  1.682510  0.137068  0.520287\n",
       "PiQA        0.706009  1.955080  1.207704  0.111297  0.188721\n",
       "QAMR        1.205043  2.712858  1.838317  0.158352  0.608850\n",
       "QuAIL       0.892183  2.226723  1.433457  0.124928  0.360089\n",
       "Quoref      1.079058  2.421550  1.641061  0.130273  0.495343\n",
       "RTE         0.713777  1.939449  1.196220  0.107657  0.179166\n",
       "SNLI        0.975155  2.431863  1.560219  0.153503  0.444826\n",
       "SocialIQA   0.838094  2.105295  1.366151  0.114203  0.311997\n",
       "WSC         0.984332  2.236511  1.547790  0.101469  0.436810\n",
       "WiC         1.020574  2.226973  1.536829  0.110745  0.429719\n",
       "Winogrande  1.005473  2.299273  1.544045  0.118759  0.434406"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = pd.concat([param_plot_stats['a'], task_name_format], axis=1)\n",
    "full = full.groupby(by='task_name').quantile(q=0.75).rename(columns={'log_mean':'Full'})\n",
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median diff (magnitude) 0.016389266234225514\n",
      "standard deviation diff (magnitude) 0.015199419535840721\n",
      "median rel diff (magnitude) 0.04174369125790653\n"
     ]
    }
   ],
   "source": [
    "combined_a = pd.concat([trimmed, full], axis=1)\n",
    "combined_a['diff'] = combined_a['Trimmed'] - combined_a['Full']\n",
    "combined_a['rel_diff'] = (combined_a['Trimmed'] - combined_a['Full'])/combined_a['Full']\n",
    "\n",
    "print('median diff (magnitude)', combined_a['diff'].abs().median())\n",
    "print('standard deviation diff (magnitude)', combined_a['diff'].abs().std())\n",
    "print('median rel diff (magnitude)', combined_a['rel_diff'].abs().median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trimmed</th>\n",
       "      <th>Full</th>\n",
       "      <th>diff</th>\n",
       "      <th>rel_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MC-TACO</th>\n",
       "      <td>0.399988</td>\n",
       "      <td>0.448584</td>\n",
       "      <td>-0.048596</td>\n",
       "      <td>-0.108332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB</th>\n",
       "      <td>0.324880</td>\n",
       "      <td>0.367416</td>\n",
       "      <td>-0.042536</td>\n",
       "      <td>-0.115771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NewsQA</th>\n",
       "      <td>0.489936</td>\n",
       "      <td>0.520287</td>\n",
       "      <td>-0.030351</td>\n",
       "      <td>-0.058335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RTE</th>\n",
       "      <td>0.154438</td>\n",
       "      <td>0.179166</td>\n",
       "      <td>-0.024728</td>\n",
       "      <td>-0.138017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARCT</th>\n",
       "      <td>0.249218</td>\n",
       "      <td>0.273343</td>\n",
       "      <td>-0.024125</td>\n",
       "      <td>-0.088259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AbductNLI</th>\n",
       "      <td>0.309183</td>\n",
       "      <td>0.330490</td>\n",
       "      <td>-0.021307</td>\n",
       "      <td>-0.064471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MNLI</th>\n",
       "      <td>0.409765</td>\n",
       "      <td>0.430545</td>\n",
       "      <td>-0.020780</td>\n",
       "      <td>-0.048265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRQA-NQ</th>\n",
       "      <td>0.507100</td>\n",
       "      <td>0.526834</td>\n",
       "      <td>-0.019734</td>\n",
       "      <td>-0.037458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANLI</th>\n",
       "      <td>0.365415</td>\n",
       "      <td>0.383340</td>\n",
       "      <td>-0.017925</td>\n",
       "      <td>-0.046759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quoref</th>\n",
       "      <td>0.480489</td>\n",
       "      <td>0.495343</td>\n",
       "      <td>-0.014854</td>\n",
       "      <td>-0.029987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNLI</th>\n",
       "      <td>0.430867</td>\n",
       "      <td>0.444826</td>\n",
       "      <td>-0.013959</td>\n",
       "      <td>-0.031381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual</th>\n",
       "      <td>0.314958</td>\n",
       "      <td>0.327908</td>\n",
       "      <td>-0.012950</td>\n",
       "      <td>-0.039492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCScript</th>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.310974</td>\n",
       "      <td>-0.011274</td>\n",
       "      <td>-0.036255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-E</th>\n",
       "      <td>0.287154</td>\n",
       "      <td>0.298371</td>\n",
       "      <td>-0.011217</td>\n",
       "      <td>-0.037595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HellaSwag</th>\n",
       "      <td>0.329438</td>\n",
       "      <td>0.338560</td>\n",
       "      <td>-0.009122</td>\n",
       "      <td>-0.026945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PiQA</th>\n",
       "      <td>0.180418</td>\n",
       "      <td>0.188721</td>\n",
       "      <td>-0.008303</td>\n",
       "      <td>-0.043995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARC-C</th>\n",
       "      <td>0.280862</td>\n",
       "      <td>0.288532</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>-0.026584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSQA</th>\n",
       "      <td>0.319349</td>\n",
       "      <td>0.326793</td>\n",
       "      <td>-0.007444</td>\n",
       "      <td>-0.022780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SocialIQA</th>\n",
       "      <td>0.305512</td>\n",
       "      <td>0.311997</td>\n",
       "      <td>-0.006486</td>\n",
       "      <td>-0.020788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MuTual+</th>\n",
       "      <td>0.353255</td>\n",
       "      <td>0.358992</td>\n",
       "      <td>-0.005737</td>\n",
       "      <td>-0.015981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuAIL</th>\n",
       "      <td>0.354671</td>\n",
       "      <td>0.360089</td>\n",
       "      <td>-0.005417</td>\n",
       "      <td>-0.015044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CosmosQA</th>\n",
       "      <td>0.330656</td>\n",
       "      <td>0.335690</td>\n",
       "      <td>-0.005033</td>\n",
       "      <td>-0.014994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QAMR</th>\n",
       "      <td>0.606264</td>\n",
       "      <td>0.608850</td>\n",
       "      <td>-0.002586</td>\n",
       "      <td>-0.004247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WiC</th>\n",
       "      <td>0.450093</td>\n",
       "      <td>0.429719</td>\n",
       "      <td>0.020374</td>\n",
       "      <td>0.047411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoolQ</th>\n",
       "      <td>0.294153</td>\n",
       "      <td>0.260289</td>\n",
       "      <td>0.033864</td>\n",
       "      <td>0.130103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Winogrande</th>\n",
       "      <td>0.468542</td>\n",
       "      <td>0.434406</td>\n",
       "      <td>0.034137</td>\n",
       "      <td>0.078582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COPA</th>\n",
       "      <td>0.232093</td>\n",
       "      <td>0.192426</td>\n",
       "      <td>0.039667</td>\n",
       "      <td>0.206141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSC</th>\n",
       "      <td>0.501799</td>\n",
       "      <td>0.436810</td>\n",
       "      <td>0.064989</td>\n",
       "      <td>0.148781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Trimmed      Full      diff  rel_diff\n",
       "task_name                                         \n",
       "MC-TACO     0.399988  0.448584 -0.048596 -0.108332\n",
       "CB          0.324880  0.367416 -0.042536 -0.115771\n",
       "NewsQA      0.489936  0.520287 -0.030351 -0.058335\n",
       "RTE         0.154438  0.179166 -0.024728 -0.138017\n",
       "ARCT        0.249218  0.273343 -0.024125 -0.088259\n",
       "AbductNLI   0.309183  0.330490 -0.021307 -0.064471\n",
       "MNLI        0.409765  0.430545 -0.020780 -0.048265\n",
       "MRQA-NQ     0.507100  0.526834 -0.019734 -0.037458\n",
       "ANLI        0.365415  0.383340 -0.017925 -0.046759\n",
       "Quoref      0.480489  0.495343 -0.014854 -0.029987\n",
       "SNLI        0.430867  0.444826 -0.013959 -0.031381\n",
       "MuTual      0.314958  0.327908 -0.012950 -0.039492\n",
       "MCScript    0.299700  0.310974 -0.011274 -0.036255\n",
       "ARC-E       0.287154  0.298371 -0.011217 -0.037595\n",
       "HellaSwag   0.329438  0.338560 -0.009122 -0.026945\n",
       "PiQA        0.180418  0.188721 -0.008303 -0.043995\n",
       "ARC-C       0.280862  0.288532 -0.007670 -0.026584\n",
       "CSQA        0.319349  0.326793 -0.007444 -0.022780\n",
       "SocialIQA   0.305512  0.311997 -0.006486 -0.020788\n",
       "MuTual+     0.353255  0.358992 -0.005737 -0.015981\n",
       "QuAIL       0.354671  0.360089 -0.005417 -0.015044\n",
       "CosmosQA    0.330656  0.335690 -0.005033 -0.014994\n",
       "QAMR        0.606264  0.608850 -0.002586 -0.004247\n",
       "WiC         0.450093  0.429719  0.020374  0.047411\n",
       "BoolQ       0.294153  0.260289  0.033864  0.130103\n",
       "Winogrande  0.468542  0.434406  0.034137  0.078582\n",
       "COPA        0.232093  0.192426  0.039667  0.206141\n",
       "WSC         0.501799  0.436810  0.064989  0.148781"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_a.loc[:,['Trimmed', 'Full', 'diff', 'rel_diff']].sort_values(by='diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trimmed</th>\n",
       "      <th>Full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trimmed</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Full</th>\n",
       "      <td>0.972188</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Trimmed      Full\n",
       "Trimmed  1.000000  0.972188\n",
       "Full     0.972188  1.000000"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_a.loc[:,['Trimmed', 'Full']].corr(method='pearson')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
